{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DA6XKs-DESD"
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_S9LVMVDESF"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/worldbank/Pakistan-Poverty-from-Sky/blob/master/DataWork/03_predict_ntl_with_dtl/02_cnn_v3.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWjucHU2DESG"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "deJsZmQSDESG"
   },
   "outputs": [],
   "source": [
    "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "# https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re \n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# import tensorflow.keras as K\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "#import keras\n",
    "#from keras.models import Sequential, Model\n",
    "#from keras import models\n",
    "#from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "#from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from keras.models import load_model, Model\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "#from keras.applications.vgg19 import VGG19\n",
    "#from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "import logging, os \n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import config as cf\n",
    "\n",
    "# Set seeds. Note that using a GPU can still introduce randomness.\n",
    "# (also not taking into account tensorflow randomness)\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9i801NpDip8",
    "outputId": "194a76fc-d9c3-4a65-f57d-fbf97ac42239"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHVApeA8GWGy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPRci3qfDESK"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7MILqyefDESK"
   },
   "outputs": [],
   "source": [
    "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        \n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size)) # dtype=int \n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load(os.path.join(NPY_PATH, ID + '.npy'))\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "        \n",
    "        return X, to_categorical(y, num_classes=self.n_classes)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6c9gwqGADESL"
   },
   "outputs": [],
   "source": [
    "def define_model_imagenet(height, width, num_classes):\n",
    "    '''\n",
    "    Defines and compiles CNN model.\n",
    "    \n",
    "    Inputs:\n",
    "        height, width, channels, num_classes (int)\n",
    "    Returns:\n",
    "        model (keras.Model object)\n",
    "    '''\n",
    "\n",
    "    # https://medium.com/abraia/first-steps-with-transfer-learning-for-custom-image-classification-with-keras-b941601fcad5\n",
    "    # https://towardsdatascience.com/cnn-transfer-learning-fine-tuning-9f3e7c5806b2\n",
    "\n",
    "    #### Base model\n",
    "    input_shape = (height, width, 3)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape, pooling = \"max\")\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #### Model Customization\n",
    "    # We take the last layer of our the model and add it to our classifier\n",
    "    last = base_model.layers[-1].output\n",
    "    x = Flatten()(last)\n",
    "    x = Dense(100, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    model = Model(base_model.input, x)\n",
    "    # We compile the model\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, training_generator, validation_generator, CNN_MODEL_CHECKPOINT):\n",
    "    '''\n",
    "    Fits model, evaluates model, saves best model over epochs and cross-validations.\n",
    "    \n",
    "    Inputs:\n",
    "        model (CNN model) keras.Model object\n",
    "        trainX, trainY (numpy.ndarray) 4D array of DTL features and 2D array of targets for training\n",
    "        testX, testY (numpy.ndarray) 4D array of DTL features and 2D array of targets for testing\n",
    "        current_kfold (int) iteration in kfold cross-val, default=None for no cross-val\n",
    "        display_metrics (bool) Default=False\n",
    "    Returns:\n",
    "        None\n",
    "    # https://towardsdatascience.com/step-by-step-guide-to-using-pretrained-models-in-keras-c9097b647b29\n",
    "    '''\n",
    "\n",
    "    # Use early stopping to help with overfitting\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=False)\n",
    "\n",
    "    # Save best model based on accuracy\n",
    "    mc = ModelCheckpoint(CNN_MODEL_CHECKPOINT, monitor='val_loss', mode='min', \n",
    "                         verbose=True, save_best_only=True)\n",
    "\n",
    "    # Fit model\n",
    "    #history = model.fit(trainX, trainY, \n",
    "    #        epochs=50, \n",
    "    #        batch_size=32, \n",
    "    #        validation_data=(testX, testY), \n",
    "    #        callbacks=[es, mc], \n",
    "    #        verbose=False)\n",
    "    \n",
    "    history = model.fit(x=training_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        use_multiprocessing=True,\n",
    "                        epochs=50,\n",
    "                        callbacks=[es, mc],\n",
    "                        workers=6)\n",
    "\n",
    "    # Show accuracy\n",
    "    #loss, accuracy = model.evaluate(testX, testY, verbose=False)\n",
    "    #print(f'                              Accuracy: {accuracy}')\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnC9uUv6DESR"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tb5r2cMgDESS"
   },
   "outputs": [],
   "source": [
    "SURVEY_NAME = 'DHS'\n",
    "SATELLITE = 'l8'\n",
    "BAND = 'BRGB'\n",
    "TARGET_VAR = 'wealth_index'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTatYzPjDEST"
   },
   "source": [
    "### Load Numpy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "n0Uxxin3DEST"
   },
   "outputs": [],
   "source": [
    "# List of npy files\n",
    "NPY_PATH = os.path.join(cf.GOOGLEDRIVE_DIRECTORY, \n",
    "             'Data', \n",
    "             SURVEY_NAME, \n",
    "             'FinalData', \n",
    "             'Individual Datasets',\n",
    "            'cnn_' + SATELLITE,\n",
    "             'npy')\n",
    "\n",
    "NPY_FILES = os.listdir(NPY_PATH)\n",
    "reg = re.compile(r'^' + BAND + '_')                  \n",
    "NPY_FILES = list(filter(reg.search, NPY_FILES)) \n",
    "\n",
    "# List of uids\n",
    "uids = [file.replace('.npy', '').replace(BAND + '_', '') for file in NPY_FILES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRB7jHDoDEST"
   },
   "source": [
    "### Prepare Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "br19bGlqDESU"
   },
   "outputs": [],
   "source": [
    "#### Load survey data\n",
    "survey_df = pd.read_csv(os.path.join(cf.DROPBOX_DIRECTORY, \n",
    "                                     'Data', \n",
    "                                     SURVEY_NAME, \n",
    "                                     'FinalData', \n",
    "                                     'Individual Datasets', \n",
    "                                     'survey_socioeconomic.csv'))\n",
    "\n",
    "#### Subset survey\n",
    "\n",
    "# Subset if target variable is NA\n",
    "survey_df = survey_df.dropna(axis=0, subset=[TARGET_VAR])\n",
    "\n",
    "# Subset to survey where we have an associated numpy array\n",
    "survey_df = survey_df[survey_df['uid'].isin(uids)]\n",
    "\n",
    "#### Variable Clean/Add\n",
    "\n",
    "# Prep target variable\n",
    "survey_df[TARGET_VAR] = np.round(survey_df[TARGET_VAR]).tolist()\n",
    "survey_df[TARGET_VAR] = survey_df[TARGET_VAR] - 1 # so starts at 0\n",
    "\n",
    "# Add band name\n",
    "survey_df['band_uid'] = BAND + '_' + survey_df['uid']\n",
    "\n",
    "# Indicate Train/Test\n",
    "survey_df['traintest'] = np.random.choice(a = ['train', 'test'], \n",
    "                                      p = [0.8, 0.2],\n",
    "                                      size = survey_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWunXASZDESU"
   },
   "source": [
    "### Dictionaries for Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5dZ6a9jKDESV"
   },
   "outputs": [],
   "source": [
    "# Partition Dictionary\n",
    "train_uids = survey_df[survey_df.traintest == 'train']['band_uid'].tolist()\n",
    "test_uids = survey_df[survey_df.traintest == 'test']['band_uid'].tolist()\n",
    "\n",
    "partition = {'train': train_uids, \n",
    "             'test': test_uids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TfSzkCAjDESV"
   },
   "outputs": [],
   "source": [
    "labels = dict(zip(survey_df.band_uid, survey_df[TARGET_VAR]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD6y2BtyDESV"
   },
   "source": [
    "## Implement CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wLJsjkysDESV"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (224,224),\n",
    "          'batch_size': 32,\n",
    "          'n_classes': 5,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], labels, **params)\n",
    "validation_generator = DataGenerator(partition['test'], labels, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dbTR0K7JDESW"
   },
   "outputs": [],
   "source": [
    "CNN_MODEL_PATH = os.path.join('/Users/robmarty/Desktop', f'CNN_DEPVAR.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxcg4gvoDESW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 19:57:55.913657 123145591713792 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/13 [==========================>...] - ETA: 15s - loss: 1.6424 - accuracy: 0.2865"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 20:01:22.897392 123145591713792 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - ETA: 0s - loss: 1.6381 - accuracy: 0.2861 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 20:01:41.596092 123145613807616 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "W0616 20:02:16.174813 123145613807616 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.60343, saving model to /Users/robmarty/Desktop/CNN_DEPVAR.h5\n",
      "13/13 [==============================] - 258s 20s/step - loss: 1.6381 - accuracy: 0.2861 - val_loss: 1.6034 - val_accuracy: 0.2708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 20:02:31.493118 123145591713792 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "12/13 [==========================>...] - ETA: 14s - loss: 1.5798 - accuracy: 0.3073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 20:05:46.667844 123145591713792 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - ETA: 0s - loss: 1.5742 - accuracy: 0.3101 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 20:06:04.568073 123145613807616 data_utils.py:537] multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    }
   ],
   "source": [
    "model = define_model_imagenet(params['dim'][0], params['dim'][1], params['n_classes'])\n",
    "evaluate_model(model, training_generator, validation_generator, CNN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzpJw5iQDESW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Isnvb2kjDESX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqqTovbODESX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sGRiFzmDESX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0igrnw6DDESX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQSSdzUfDESX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrH_V9zLDESX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kig8xI3KDESX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Zf9GwEUDESX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-3igWDBDESY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3GZM2yoDESY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxqg6RUhDESY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8eiALrqDESY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUuSTN1rDESY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oa4mxIGjDESY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5I-8xwD3DESY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jGOsAnZDESY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NilJBRG5DESY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feckQ8HjDESZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFKsG357DESZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IW7a55A6DESZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_cnn_v3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
