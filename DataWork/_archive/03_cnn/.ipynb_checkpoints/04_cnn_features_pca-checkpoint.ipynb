{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_grid.py\n",
    "#\n",
    "# Description:\n",
    "# Performs complete process of cleaning data and prepping all necessary data \n",
    "# and running the grid search.\n",
    "\n",
    "# https://towardsdatascience.com/extract-features-visualize-filters-and-feature-maps-in-vgg16-and-vgg19-cnn-models-d2da6333edd0\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## User Defined\n",
    "import config as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ea26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca_n(df, n):\n",
    "    '''\n",
    "    Performs PCA with n compponents on all columns in df.\n",
    "    '''\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(df)\n",
    "    features_pca = pca.transform(df)\n",
    "    column_names = ['cnn_pc_%01d' %i for i in range(0,n)]\n",
    "    df_features_pca = pd.DataFrame(data=features_pca, columns=column_names)\n",
    "    return df_features_pca, pca.explained_variance_ratio_\n",
    "\n",
    "def perform_pca_expln(df, prop):\n",
    "    '''\n",
    "    Performs PCA with n compponents on all columns in df.\n",
    "    '''\n",
    "    pca = PCA(prop)\n",
    "    pca.fit(df)\n",
    "    features_pca = pca.transform(df)\n",
    "\n",
    "    n = features_pca.shape[1]\n",
    "\n",
    "    column_names = ['cnn_pc_%01d' %i for i in range(0,n)]\n",
    "    df_features_pca = pd.DataFrame(data=features_pca, columns=column_names)\n",
    "    \n",
    "    return df_features_pca, pca.explained_variance_ratio_\n",
    "\n",
    "def extract_pca_features(df):\n",
    "\n",
    "    n_components = 10\n",
    "    \n",
    "    # initialize a dictionary to store 5 dataframes, the extracted features per CNN model\n",
    "    pca_dict = {}\n",
    "    \n",
    "    # create a list of suffixes to append to the column names in order to differentiate between features for all five models\n",
    "    suffix = [\"_Nbands3\",\"_Band1\",\"_Band5\",\"_Band6\",\"_Band7\"]\n",
    "    \n",
    "    # create a list of column names that are from the OPM data, not part of the cnn features\n",
    "    column_names = pd.Series(df.columns)\n",
    "    opm_cols = column_names[~column_names.str.contains('cnn_feat', regex=False)]\n",
    "    cols = [cols for cols in opm_cols]\n",
    "    \n",
    "    # for each channel band (RGB plus single channels), filter the dataframe to just include cnn features \n",
    "    # for the specific channel at each iteration. Run PCA on the filtered or subsetted data and store the resulted dataframe\n",
    "    # in a dictionary with suffix as the key. Add the suffix to the column names in order to differentiate between features 0 through \n",
    "    # 99 for all five models. Concatenated the dictionary to one large dataframe and add the columns from OPM\n",
    "    # data back into the new dataframe. \n",
    "    for i in range(len(suffix)):\n",
    "        df2 = df.filter(regex=suffix[i])\n",
    "        pca_df, expl_var = perform_pca_expln(df2, 0.9)\n",
    "        pca_dict[suffix[i]] = pca_df\n",
    "        pca_dict[suffix[i]] = pca_dict[suffix[i]].add_suffix(suffix[i])\n",
    "    df_pca = pd.concat(pca_dict, axis=1)\n",
    "    df_pca.columns = df_pca.columns.get_level_values(1)\n",
    "    df_pca[cols] = df[cols]\n",
    "    \n",
    "    # 5. Export           \n",
    "    df_pca.to_pickle(os.path.join('s3://worldbank-pakistan-data', 'OPM' , 'FinalData', 'Merged Datasets', 'cnn_merge_pca.pkl'))\n",
    "    df_pca.to_csv(os.path.join('s3://worldbank-pakistan-data', 'OPM' , 'FinalData', 'Merged Datasets', 'cnn_merge_pca.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee312a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "df = pd.read_csv(os.path.join('s3://worldbank-pakistan-data', 'OPM' , 'FinalData', 'Merged Datasets', 'cnn_merge.csv'))\n",
    "extract_pca_features(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
