{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Satellite Imagery to Survey Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "#ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geetools\n",
    "from geetools import ui, cloud_mask\n",
    "import os, datetime\n",
    "import glob\n",
    "import config as cf\n",
    "import pandas as pd\n",
    "import time\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "#import eeconvert\n",
    "\n",
    "cloud_mask_landsatSR = cloud_mask.landsatSR()\n",
    "cloud_mask_sentinel2 = cloud_mask.sentinel2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURVEY_NAME = 'DHS'\n",
    "REEXTRACT_IF_FILE_EXISTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gis.stackexchange.com/questions/257727/iterate-over-imagecollection-returning-pandas-dataframe-using-earth-engine-pyt\n",
    "def fc2df(fc):\n",
    "    # Convert a FeatureCollection into a pandas DataFrame\n",
    "    # Features is a list of dict with the output\n",
    "    features = fc.getInfo()['features']\n",
    "\n",
    "    dictarr = []\n",
    "\n",
    "    for f in features:\n",
    "        # Store all attributes in a dict\n",
    "        attr = f['properties']\n",
    "        # and treat geometry separately\n",
    "        attr['geometry'] = f['geometry']  # GeoJSON Feature!\n",
    "        # attr['geometrytype'] = f['geometry']['type']\n",
    "        dictarr.append(attr)\n",
    "\n",
    "    df = gpd.GeoDataFrame(dictarr)\n",
    "    # Convert GeoJSON features to shape\n",
    "    df = df.drop(columns=['geometry'])\n",
    "    return df\n",
    "\n",
    "def survey_to_fc_buffer(survey_df, buffer_size_urban, buffer_size_rural):\n",
    "    '''\n",
    "    Convert pandas dataframe of survey locations to a feature collection. \n",
    "    \n",
    "    Inputs:\n",
    "        survey_df: pandas dataframe of survey locations. Function assumes \n",
    "                   the dataframe contains (1) latitude, (2) longitude and\n",
    "                   (3) uid variables. Assumes coordinates in WGS84.\n",
    "    Returns:\n",
    "        (feature collection)\n",
    "    '''\n",
    "    \n",
    "    POLLUTION_SCALE = 10000\n",
    "    \n",
    "    survey_fc_list = []\n",
    "    \n",
    "    n_rows = survey_df.shape[0]\n",
    "    for i in range(0, n_rows):\n",
    "        survey_df_i = survey_df.iloc[[i]]\n",
    "        \n",
    "        #ur = survey_df_i['urban_rural'].iloc[0]\n",
    "        #if ur == 'U':\n",
    "        #    buffer_size = buffer_size_urban\n",
    "        #elif ur == 'R':\n",
    "        #    buffer_size = buffer_size_rural\n",
    "        buffer_size = buffer_size_urban\n",
    "\n",
    "        f_i = ee.Feature(ee.Geometry.Point([survey_df_i['longitude'].iloc[0], \n",
    "                                            survey_df_i['latitude'].iloc[0]]), \n",
    "                         {'uid': survey_df_i['uid'].iloc[0],\n",
    "                          'year': str(survey_df_i['year'].iloc[0])})\n",
    "        \n",
    "        f_i = f_i.buffer(buffer_size)\n",
    "\n",
    "        survey_fc_list.append(f_i)\n",
    "        \n",
    "    survey_fc = ee.FeatureCollection(survey_fc_list)\n",
    "    \n",
    "    return survey_fc\n",
    "\n",
    "def extract_sat(survey_df, buffer_size_urban, buffer_size_rural, year, satellite, survey_name, file_name):\n",
    "    '''\n",
    "    Extract satellite imagery to locations \n",
    "    \n",
    "    Inputs:\n",
    "        survey_df: pandas dataframe of survey locations. Function assumes \n",
    "                   the dataframe contains (1) latitude, (2) longitude and\n",
    "                   (3) uid variables. Assumes coordinates in WGS84.\n",
    "    Returns:\n",
    "        (feature collection)\n",
    "    '''\n",
    "    \n",
    "    POLLUTION_SCALE = 10000 # HERE!\n",
    "        \n",
    "    year_start_sp5 = \"2018-01-01\"\n",
    "    year_end_sp5 = '2020-12-31'\n",
    "    \n",
    "    # Prep worldpop -----------------------------------------------\n",
    "    if satellite == 'worldpop':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 100 \n",
    "        \n",
    "        # Year\n",
    "        year_use = max([2000, year])\n",
    "                \n",
    "        year_plus = year_use\n",
    "        year_minus = year_use\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('WorldPop/GP/100m/pop')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        # After the reducer computers the sum, it names the value \"sum\", not population\n",
    "        BANDS = ['sum']\n",
    "        \n",
    "    # Prep worldpop_2020 ---------------------------------------------\n",
    "    if satellite == 'worldpop2020':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 100 \n",
    "        \n",
    "        # Year\n",
    "        year_use = '2020'\n",
    "        \n",
    "        year_plus = year_use\n",
    "        year_minus = year_use\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('WorldPop/GP/100m/pop')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        # After the reducer computers the sum, it names the value \"sum\", not population\n",
    "        BANDS = ['sum']\n",
    "            \n",
    "    # Sentinel-5P OFFL AER AI: Offline UV Aerosol Index  -------------------\n",
    "    if satellite == 'uv_aer':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_AER_AI\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['absorbing_aerosol_index']\n",
    "        \n",
    "    # Sentinel-5P OFFL CO: Offline Carbon Monoxide  -------------------\n",
    "    if satellite == 'CO':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['CO_column_number_density', 'H2O_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL HCHO: Offline Formaldehyde  -------------------\n",
    "    if satellite == 'HCHO':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_HCHO\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['tropospheric_HCHO_column_number_density', 'tropospheric_HCHO_column_number_density_amf']\n",
    "        \n",
    "    # Sentinel-5P Nitrogen Dioxide  -----------------------------\n",
    "    if satellite == 'NO2':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_NO2\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['NO2_column_number_density', 'tropospheric_NO2_column_number_density',\\\n",
    "                 'stratospheric_NO2_column_number_density', 'NO2_slant_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL O3: Offline Ozone  -------------------\n",
    "    if satellite == 'ozone':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_O3\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['O3_column_number_density', 'O3_effective_temperature']\n",
    "        \n",
    "    # Sentinel-5P OFFL SO2: Offline Sulphur Dioxide  -------------------\n",
    "    if satellite == 'SO2':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_SO2\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['SO2_column_number_density', 'SO2_column_number_density_amf', 'SO2_slant_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL CH4: Offline Methane  -------------------\n",
    "    if satellite == 'CH4':\n",
    "        \n",
    "        # Scale\n",
    "        #SCALE = 1113.2 # takes too long\n",
    "        #SCALE = 10000\n",
    "        SCALE = POLLUTION_SCALE\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CH4\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['CH4_column_volume_mixing_ratio_dry_air']\n",
    "        \n",
    "    # CSP gHM: Global Human Modification ---------------------------------\n",
    "    if satellite == 'GlobalHumanModification':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.ImageCollection(\"CSP/HM/GlobalHumanModification\")\\\n",
    "            .median()\n",
    "        \n",
    "        # Original name is \"gHM\", but because only one value, it takes the\n",
    "        # name of the reducer; we use mean\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # WorldClim BIO Variables V1 ---------------------------------\n",
    "    if satellite == 'worldclim_bio':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.Image('WORLDCLIM/V1/BIO')\n",
    "        \n",
    "        BANDS = ['bio01', 'bio02', 'bio03', 'bio04', 'bio05', 'bio06', 'bio07', 'bio08', 'bio09', 'bio10',\\\n",
    "                 'bio11', 'bio12', 'bio13', 'bio14', 'bio15', 'bio16', 'bio17', 'bio18', 'bio19']\n",
    "        \n",
    "    # Elevation - SRTM ------------------------------------------\n",
    "    if satellite == 'elevation':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.Image('USGS/SRTMGL1_003') # CGIAR/SRTM90_V4\n",
    "        \n",
    "        # elevation?\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # Elevation - SRTM ------------------------------------------\n",
    "    if satellite == 'slope':\n",
    "        # https://developers.google.com/earth-engine/datasets/catalog/CGIAR_SRTM90_V4#description\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 500 # ok to upscale\n",
    "                \n",
    "        image_raw = ee.Image('USGS/SRTMGL1_003') # CGIAR/SRTM90_V4\n",
    "        image_elev = image_raw.select('elevation')\n",
    "        image = ee.Terrain.slope(image_elev)\n",
    "                \n",
    "        # mean?\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # Prep l5 ---------------------------------------------------\n",
    "    if satellite == 'l5':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        ### Year\n",
    "        # (1) landsat 5 starts in March 1984; if year is less than\n",
    "        #     1985, use 1985 as year (to ensure have year before and after)\n",
    "        # (2) landsat 5 ends in May 2012; if year is greater than\n",
    "        #     2011, use 2011 as year\n",
    "        if year < 1985:\n",
    "            year_use = 1985\n",
    "        elif year > 2011:\n",
    "            year_use = 2011\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        #image = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2')\\\n",
    "        #    .filterDate(year_minus_str, year_plus_str)\\\n",
    "        #    #.map(cloud_mask_landsatSR)\\\n",
    "        #    .median()\\\n",
    "        #    .multiply(0.0001)\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "        ndvi = image.normalizedDifference(['B4', 'B3']).rename('NDVI');\n",
    "        ndbi = image.normalizedDifference(['B5', 'B4']).rename('NDBI');\n",
    "        image = image.addBands(ndvi)\n",
    "        image = image.addBands(ndbi)\n",
    "        \n",
    "        bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "        image = image.addBands(bu)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'NDVI', 'NDBI', 'BU']\n",
    "        \n",
    "    # Prep l7 ---------------------------------------------------\n",
    "    if (satellite == 'l7') | (satellite == 'l7_sdspace'):\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 7 starts in May 1999; if year is less than\n",
    "        # 2000, use 2000 as year (to ensure have year before and after)\n",
    "        if year < 2000:\n",
    "            year_use = 2000\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "        ndvi = image.normalizedDifference(['B4', 'B3']).rename('NDVI');\n",
    "        ndbi = image.normalizedDifference(['B5', 'B4']).rename('NDBI');\n",
    "        image = image.addBands(ndvi)\n",
    "        image = image.addBands(ndbi)\n",
    "        \n",
    "        bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "        image = image.addBands(bu)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'NDVI', 'NDBI', 'BU']\n",
    "        \n",
    "    # Prep l7 ---------------------------------------------------\n",
    "    if satellite == 'l7_sdtime':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 7 starts in May 1999; if year is less than\n",
    "        # 2000, use 2000 as year (to ensure have year before and after)\n",
    "        if year < 2000:\n",
    "            year_use = 2000\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .reduce(ee.Reducer.stdDev())\n",
    "        \n",
    "        BANDS = ['B1_stdDev', 'B2_stdDev', 'B3_stdDev', 'B4_stdDev', 'B5_stdDev', 'B6_stdDev', 'B7_stdDev']\n",
    "                \n",
    "    # Prep l8 ---------------------------------------------------\n",
    "    if (satellite == 'l8') | (satellite == 'l8_sdspace'):\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 8 starts in April 2013; if year is less than\n",
    "        # 2014, use 2014 as year (to ensure have year before and after)\n",
    "        if year < 2014:\n",
    "            year_use = 2014\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "        ndvi = image.normalizedDifference(['B5', 'B4']).rename('NDVI');\n",
    "        ndbi = image.normalizedDifference(['B6', 'B5']).rename('NDBI');\n",
    "        image = image.addBands(ndvi)\n",
    "        image = image.addBands(ndbi)\n",
    "        \n",
    "        bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "        image = image.addBands(bu)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'NDVI', 'NDBI', 'BU']\n",
    "        \n",
    "    # Prep l8 ---------------------------------------------------\n",
    "    if satellite == 'l8_sdtime':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 8 starts in April 2013; if year is less than\n",
    "        # 2014, use 2014 as year (to ensure have year before and after)\n",
    "        if year < 2014:\n",
    "            year_use = 2014\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .reduce(ee.Reducer.stdDev())\n",
    "        \n",
    "        BANDS = ['B1_stdDev', 'B2_stdDev', 'B3_stdDev', 'B4_stdDev', 'B5_stdDev', 'B6_stdDev', 'B7_stdDev', 'B10_stdDev', 'B11_stdDev']\n",
    "                \n",
    "    # Prep s2 ---------------------------------------------------\n",
    "    if satellite == 's2':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        \n",
    "        # Year\n",
    "        # sentinel starts in March 2017; juse use 2018\n",
    "        year_use = 2018\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-12-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('COPERNICUS/S2_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_sentinel2)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "        \n",
    "        ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI');\n",
    "        image = image.addBands(ndvi)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'AOT', 'NDVI']\n",
    "\n",
    "        image = image.select(BANDS) \n",
    "      \n",
    "    # Prep SAR Median - HH/HV DESC ---------------------------------------------------\n",
    "    # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S1_GRD\n",
    "    if 's1_sar' in satellite:\n",
    "        \n",
    "        SCALE = 10 # ok to upscale\n",
    "    \n",
    "        ## H/V\n",
    "        if 'hh' in satellite:\n",
    "            HV_VAR = 'HH'\n",
    "            \n",
    "        if 'hv' in satellite:\n",
    "            HV_VAR = 'HV'\n",
    "            \n",
    "        if 'vv' in satellite:\n",
    "            HV_VAR = 'VV'\n",
    "            \n",
    "        if 'vh' in satellite:\n",
    "            HV_VAR = 'VH'\n",
    "            \n",
    "        if 'vdiv' in satellite:\n",
    "            HV_VAR = 'VV_DIV_VH'\n",
    "            \n",
    "        ## A/D\n",
    "        if 'desc' in satellite:\n",
    "            AD_VAR = 'DESCENDING'\n",
    "            \n",
    "        if 'asc' in satellite:\n",
    "            AD_VAR = 'ASCENDING'\n",
    "            \n",
    "        ## Year\n",
    "        year_use = 2018\n",
    "            \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        ## Image\n",
    "        if HV_VAR == 'VV_DIV_VH':\n",
    "            \n",
    "            image_vv = ee.ImageCollection('COPERNICUS/S1_GRD')\\\n",
    "                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\\\n",
    "                .filter(ee.Filter.eq('instrumentMode', 'IW'))\\\n",
    "                .filter(ee.Filter.eq('orbitProperties_pass', AD_VAR))\\\n",
    "                .select('VV')\\\n",
    "                .filterDate(year_minus_str, year_plus_str)\\\n",
    "                .mean()\n",
    "\n",
    "            image_vh = ee.ImageCollection('COPERNICUS/S1_GRD')\\\n",
    "                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\\\n",
    "                .filter(ee.Filter.eq('instrumentMode', 'IW'))\\\n",
    "                .filter(ee.Filter.eq('orbitProperties_pass', AD_VAR))\\\n",
    "                .select('VH')\\\n",
    "                .filterDate(year_minus_str, year_plus_str)\\\n",
    "                .mean()\n",
    "\n",
    "            image = image_vv.divide(image_vh)\n",
    "        \n",
    "        else: \n",
    "            image = ee.ImageCollection('COPERNICUS/S1_GRD')\\\n",
    "                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', HV_VAR))\\\n",
    "                .filter(ee.Filter.eq('instrumentMode', 'IW'))\\\n",
    "                .filter(ee.Filter.eq('orbitProperties_pass', AD_VAR))\\\n",
    "                .select(HV_VAR)\\\n",
    "                .filterDate(year_minus_str, year_plus_str)\\\n",
    "                .mean()\n",
    "\n",
    "        ## Mean / Std Dev\n",
    "        #if 'mean' in satellite:\n",
    "        #    image = image.mean()\n",
    "            \n",
    "        #if 'stddev' in satellite:\n",
    "        #    image = image.reduce(ee.Reducer.stdDev())\n",
    "            \n",
    "        BANDS = ['mean']\n",
    "                \n",
    "\n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'gridmet_drought':\n",
    "        \n",
    "        SCALE = 5000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"GRIDMET/DROUGHT\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['pdsi', 'z', 'eddi1y', 'eddi2y', 'eddi5y']\n",
    "        \n",
    "    # Prep AOD ------------------------------------------------------\n",
    "    if satellite == 'aod':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        year_use = max([2001, year])\n",
    "\n",
    "        year_minus_str = str(year_use) + '-01-01'\n",
    "        year_plus_str = str(year_use) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"MODIS/006/MCD19A2_GRANULES\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['Optical_Depth_047', 'Optical_Depth_055']\n",
    "    \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        # Data available until July 2020\n",
    "        year_use = min([2019, year])\n",
    "        \n",
    "        year_minus_str = str(year_use) + '-01-01'\n",
    "        year_plus_str = str(year_use) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q1':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Data available until July 2020\n",
    "        year_use = min([2019, year])\n",
    "\n",
    "        year_minus_str = str(year_use) + '-01-01'\n",
    "        year_plus_str = str(year_use) + '-03-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q2':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Data available until July 2020\n",
    "        year_use = min([2019, year])\n",
    "\n",
    "        year_minus_str = str(year_use) + '-04-01'\n",
    "        year_plus_str = str(year_use) + '-06-30'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q3':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Data available until July 2020\n",
    "        year_use = min([2019, year])\n",
    "\n",
    "        year_minus_str = str(year_use) + '-07-01'\n",
    "        year_plus_str = str(year_use) + '-09-30'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q4':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Data available until July 2020\n",
    "        year_use = min([2019, year])\n",
    "\n",
    "        year_minus_str = str(year_use) + '-10-01'\n",
    "        year_plus_str = str(year_use) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "    \n",
    "    # Prep viirs ---------------------------------------------------\n",
    "    if (satellite == 'viirs') | (satellite == 'viirs_sdspace'):\n",
    "        \n",
    "        SCALE = 500 \n",
    "        \n",
    "        # Year\n",
    "        # VIIRS starts in April 2012; if year is less than\n",
    "        # 2013, use 2013 as year (to ensure have year before and after)\n",
    "        if year < 2013:\n",
    "            year_use = 2013\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['avg_rad']\n",
    "        \n",
    "    # Prep viirs ---------------------------------------------------\n",
    "    # https://gis.stackexchange.com/questions/344626/gee-pixel-based-sd-over-time-series-sentinel-2-ndvi\n",
    "    if satellite == 'viirs_sdtime':\n",
    "        \n",
    "        SCALE = 500 \n",
    "        \n",
    "        # Year\n",
    "        # VIIRS starts in April 2012; if year is less than\n",
    "        # 2013, use 2013 as year (to ensure have year before and after)\n",
    "        if year < 2013:\n",
    "            year_use = 2013\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .reduce(ee.Reducer.stdDev())\n",
    "        \n",
    "        BANDS = ['avg_rad_stdDev']\n",
    "        \n",
    "    # Prep viirs181920 ---------------------------------------------------\n",
    "    if satellite == 'viirs181920':\n",
    "        \n",
    "        SCALE = 500 \n",
    "        \n",
    "        # Year\n",
    "        # VIIRS starts in April 2012; if year is less than\n",
    "        # 2013, use 2013 as year (to ensure have year before and after)\n",
    "        year_use = 2019\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['avg_rad']\n",
    "        \n",
    "    # Prep DMSP ---------------------------------------------------\n",
    "    if satellite == 'dmsp':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Year\n",
    "        # DMSP-OLS starts in 2013; if year is more than\n",
    "        # 2012, use 2012 as year (to ensure have year before and after)\n",
    "        if year > 2012:\n",
    "            year_use = 2012\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/DMSP-OLS/NIGHTTIME_LIGHTS')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['stable_lights', 'avg_lights_x_pct']\n",
    "    \n",
    "    # Prep Survey ---------------------------------------------------\n",
    "    survey_fc = survey_to_fc_buffer(survey_df, buffer_size_urban, buffer_size_rural)\n",
    "        \n",
    "    # Extract Values ---------------------------------------------------\n",
    "    if (satellite == 'worldpop') | (satellite == 'worldpop2020'):\n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.sum(),\n",
    "                                   scale = SCALE,\n",
    "                                   tileScale = 8)\n",
    "    elif (satellite == 'viirs_sdspace') | (satellite == 'l7_sdspace') | (satellite == 'l8_sdspace') | ( ('s1_sar' in satellite) & ('stddev' in satellite) ):\n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                           reducer = ee.Reducer.stdDev(),\n",
    "                           scale = SCALE,\n",
    "                           tileScale = 8)   \n",
    "    elif (satellite == 'NO2') | (satellite == 'uv_aer') | (satellite == 'CO') | (satellite == 'HCHO') | (satellite == 'ozone') | (satellite == 'SO2') | (satellite == 'CH4'):\n",
    "        \n",
    "        #vals = survey_fc.map(lambda feature: ee.Feature(None, image.reduceRegion(\n",
    "        #    reducer=ee.Reducer.mean(),\n",
    "        #    geometry=feature.geometry(),\n",
    "        #    scale=SCALE,\n",
    "        #    bestEffort = True\n",
    "        #)))\n",
    "        \n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.mean(),\n",
    "                                   scale = SCALE) \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.mean(),\n",
    "                                   scale = SCALE,\n",
    "                                   tileScale = 8) \n",
    "        \n",
    "\n",
    "        \n",
    "    # OLD =============\n",
    "    # Survey dataset that only contains the uid variable\n",
    "    #survey_df = survey_df[['uid']]\n",
    "            \n",
    "    #for band_i in BANDS:\n",
    "    #    survey_df[satellite + '_' + band_i] = vals.aggregate_array(band_i).getInfo()\n",
    "        \n",
    "    # NEW =============\n",
    "    #df_out = fc2df(vals)\n",
    "    #print(df_out)\n",
    "    #df_out = pd.DataFrame()\n",
    "    \n",
    "    bands_to_export = BANDS.copy()\n",
    "    bands_to_export.append('uid')\n",
    "    bands_to_export.append('year')\n",
    "    #print(bands_to_export)\n",
    "    \n",
    "    task = ee.batch.Export.table.toDrive(collection=vals, \n",
    "                                         folder='satellite_data_from_gee_' + survey_name.lower(), \n",
    "                                         description=file_name, \n",
    "                                         fileFormat='CSV',\n",
    "                                         selectors = bands_to_export)\n",
    "    \n",
    "    #task = ee.batch.Export.table.toDrive(collection=vals, \n",
    "    #                                     folder='gee_outputs_dhs', \n",
    "    #                                     description=file_name, \n",
    "    #                                     fileFormat='CSV',\n",
    "    #                                     selectors = bands_to_export)\n",
    "     #selectors=props\n",
    "    task.start()\n",
    "    #ee.batch.data.startProcessing(mytask.id, mytask.config)\n",
    "    \n",
    "    if False:\n",
    "        time_elapsed = 0\n",
    "        while task.active():\n",
    "            if((time_elapsed % 60) == 0):\n",
    "                print('Polling for task (id: {}).'.format(task.id))\n",
    "            time.sleep(5)\n",
    "            time_elapsed = time_elapsed + 5\n",
    "        \n",
    "    return task\n",
    "\n",
    "def extract_satellite_in_chunks(survey_df, buffer_size_urban, buffer_size_rural, satellite, file_name, year, survey_name):\n",
    "    \n",
    "    vals_df_list = []\n",
    "    \n",
    "    for chunk_i in list(np.unique(survey_df.chunk_id)):\n",
    "\n",
    "        survey_df_i = survey_df[survey_df['chunk_id'] == chunk_i]\n",
    "        vals_i_df = extract_sat(survey_df_i, buffer_size_urban, buffer_size_rural, satellite, file_name, year, chunk_i, survey_name)\n",
    "\n",
    "        vals_df_list.append(vals_i_df)\n",
    "    \n",
    "    return vals_df_list\n",
    "\n",
    "def extract_satellite_by_year(survey_df, buffer_size_urban, buffer_size_rural, satellite, file_name, survey_name):\n",
    "    \n",
    "    vals_df_list = []\n",
    "    \n",
    "    for year_i in list(np.unique(survey_df.year)):\n",
    "\n",
    "        survey_df_i = survey_df[survey_df['year'] == year_i]\n",
    "        vals_i_df = extract_satellite_in_chunks(survey_df_i, buffer_size_urban, buffer_size_rural, satellite, file_name, year_i, survey_name)\n",
    "\n",
    "        vals_df_list.append(vals_i_df)\n",
    "    \n",
    "    return vals_df_list\n",
    "\n",
    "def chunk_ids(total_length, chunk_size):\n",
    "    n_numbers = np.ceil(total_length / chunk_size)\n",
    "    n_numbers = int(n_numbers)\n",
    "    \n",
    "    chunk_ids = list(range(0,n_numbers)) * chunk_size\n",
    "    chunk_ids.sort()\n",
    "    chunk_ids = chunk_ids[:total_length]\n",
    "    \n",
    "    return chunk_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/Prep Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv(os.path.join(cf.DROPBOX_DIRECTORY, 'Data', SURVEY_NAME, 'FinalData', 'Individual Datasets', 'survey_socioeconomic.csv'))\n",
    "survey_df = survey_df[['uid', 'year', 'latitude', 'longitude', 'most_recent_survey']] # urban_rural\n",
    "survey_df = survey_df.sort_values('year')\n",
    "#survey_df = survey_df.head(1000)\n",
    "#survey_df = survey_df[survey_df.uid != 'IA201400180012']\n",
    "\n",
    "survey_years = list(survey_df.year.unique())\n",
    "\n",
    "#CHUNK_SIZE = 1000\n",
    "#survey_df['chunk_id'] = 0chunk_ids(survey_df.shape[0], CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  uid  year   latitude  longitude  most_recent_survey\n",
      "3540   BF199900000002  1998  12.515376  -1.690597               False\n",
      "76814  TG199800000137  1998   6.235644   1.472263               False\n",
      "76815  TG199800000138  1998   6.770265   1.512131               False\n",
      "76816  TG199800000139  1998   6.708370   1.491649               False\n",
      "19776  GH199800000130  1998   5.653637   0.024108               False\n",
      "(82227, 5)\n"
     ]
    }
   ],
   "source": [
    "print(survey_df.head())\n",
    "print(survey_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If re-extract, delete existing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REEXTRACT_IF_FILE_EXISTS:\n",
    "    print(\"Deleting existing files from Google Drive\")\n",
    "\n",
    "    ## Path with files\n",
    "    OUT_PATH = os.path.join(cf.GOOGLEDRIVE_DIRECTORY, \n",
    "                            'Data', \n",
    "                             SURVEY_NAME, \n",
    "                             'FinalData', \n",
    "                             'Individual Datasets',\n",
    "                             'satellite_data_from_gee_' + SURVEY_NAME.lower())\n",
    "\n",
    "    ## Grab csv files\n",
    "    files_to_rm = [x for x in os.listdir(OUT_PATH) if '.csv' in x]\n",
    "\n",
    "    ## Delete files\n",
    "    for file_i in files_to_rm:\n",
    "\n",
    "        path_i = os.path.join(OUT_PATH, file_i)\n",
    "        os.remove(path_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of files already extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1544"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Path with files\n",
    "OUT_PATH = os.path.join(cf.GOOGLEDRIVE_DIRECTORY, \n",
    "                        'Data', \n",
    "                         SURVEY_NAME, \n",
    "                         'FinalData', \n",
    "                         'Individual Datasets',\n",
    "                         'satellite_data_from_gee_' + SURVEY_NAME.lower())\n",
    "\n",
    "## Grab csv files\n",
    "files_extracted = [x for x in os.listdir(OUT_PATH) if '.csv' in x]\n",
    "\n",
    "len(files_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if (file_name_i_csv not in files_extracted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gee_s1_sar_vdiv_asc_stddev_ubuff2500_rbuff2500_2020_16.csv'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name_i_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name_i_csv not in files_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"file\": check if file exists\n",
    "# \"data\": check processed data\n",
    "\n",
    "how_check_processed = 'file' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1_sar_vv_desc_mean\n",
      "s1_sar_vv_desc_stddev\n",
      "s1_sar_vv_asc_mean\n",
      "s1_sar_vv_asc_stddev\n",
      "s1_sar_vh_desc_mean\n",
      "s1_sar_vh_desc_stddev\n",
      "s1_sar_vh_asc_mean\n",
      "s1_sar_vh_asc_stddev\n",
      "s1_sar_vdiv_desc_mean\n",
      "gee_s1_sar_vdiv_desc_mean_ubuff2500_rbuff2500_2015_9.csv\n",
      "gee_s1_sar_vdiv_desc_mean_ubuff2500_rbuff2500_2015_10.csv\n",
      "gee_s1_sar_vdiv_desc_mean_ubuff2500_rbuff2500_2015_11.csv\n",
      "gee_s1_sar_vdiv_desc_mean_ubuff2500_rbuff2500_2015_12.csv\n",
      "gee_s1_sar_vdiv_desc_mean_ubuff2500_rbuff2500_2017_14.csv\n",
      "s1_sar_vdiv_desc_stddev\n",
      "gee_s1_sar_vdiv_desc_stddev_ubuff2500_rbuff2500_2002_1.csv\n",
      "gee_s1_sar_vdiv_desc_stddev_ubuff2500_rbuff2500_2003_2.csv\n",
      "gee_s1_sar_vdiv_desc_stddev_ubuff2500_rbuff2500_2005_2.csv\n",
      "gee_s1_sar_vdiv_desc_stddev_ubuff2500_rbuff2500_2008_3.csv\n",
      "gee_s1_sar_vdiv_desc_stddev_ubuff2500_rbuff2500_2010_5.csv\n",
      "gee_s1_sar_vdiv_desc_stddev_ubuff2500_rbuff2500_2011_5.csv\n",
      "gee_s1_sar_vdiv_desc_stddev_ubuff2500_rbuff2500_2014_6.csv\n",
      "gee_s1_sar_vdiv_desc_stddev_ubuff2500_rbuff2500_2015_7.csv\n",
      "gee_s1_sar_vdiv_desc_stddev_ubuff2500_rbuff2500_2015_8.csv\n",
      "gee_s1_sar_vdiv_desc_stddev_ubuff2500_rbuff2500_2015_10.csv\n",
      "gee_s1_sar_vdiv_desc_stddev_ubuff2500_rbuff2500_2015_12.csv\n",
      "s1_sar_vdiv_asc_mean\n",
      "s1_sar_vdiv_asc_stddev\n"
     ]
    }
   ],
   "source": [
    "to_extract = ['elevation', \n",
    "              'slope',\n",
    "              #'viirs_750',\n",
    "              'viirs_1120',\n",
    "              #'viirs_1250',\n",
    "              #'viirs_1500',\n",
    "              #'viirs_2000',\n",
    "              'viirs_2500',\n",
    "              'viirs_3360',\n",
    "              #'viirs_5000',\n",
    "              #'viirs181920_750',\n",
    "              'viirs181920_1120',\n",
    "              #'viirs181920_1250',\n",
    "              #'viirs181920_1500',\n",
    "              #'viirs181920_2000',\n",
    "              #'viirs181920_2500',\n",
    "              'viirs181920_3360',\n",
    "              #'viirs181920_5000',\n",
    "              'viirs_sdtime_2500',\n",
    "              'viirs_sdspace_2500',\n",
    "              'GlobalHumanModification',\n",
    "              #'worldpop_750',\n",
    "              #'worldpop_1500',\n",
    "              'worldpop_2000',\n",
    "              #'worldpop_2500',\n",
    "              'worldpop_5000',\n",
    "              'worldpop_10000',\n",
    "              #'worldpop2020_750',\n",
    "              #'worldpop2020_1500',\n",
    "              'worldpop2020_2000',\n",
    "              #'worldpop2020_2500',\n",
    "              'worldpop2020_5000',\n",
    "              'worldpop2020_10000',\n",
    "              'l8',\n",
    "              'l7',\n",
    "              'l8_sdtime',\n",
    "              'l7_sdtime',\n",
    "              'l8_sdspace',\n",
    "              'l7_sdspace',\n",
    "              'aod',\n",
    "              'ecmwf_weather',\n",
    "              'ecmwf_weather_q1',\n",
    "              'ecmwf_weather_q2',\n",
    "              'ecmwf_weather_q3',\n",
    "              'ecmwf_weather_q4',\n",
    "             's1_sar_h_med', \n",
    "              's1_sar_h_stddev']\n",
    "\n",
    "to_extract = ['s1_sar_hh_desc_mean',\n",
    "             's1_sar_hh_desc_stddev',\n",
    "             's1_sar_hh_asc_mean',\n",
    "             's1_sar_hh_asc_stddev',\n",
    "             's1_sar_vv_desc_mean',\n",
    "             's1_sar_vv_desc_stddev',\n",
    "             's1_sar_vv_asc_mean',\n",
    "             's1_sar_vv_asc_stddev',\n",
    "             's1_sar_hv_desc_mean',\n",
    "             's1_sar_hv_desc_stddev',\n",
    "             's1_sar_hv_asc_mean',\n",
    "             's1_sar_hv_asc_stddev',\n",
    "             's1_sar_vh_desc_mean',\n",
    "             's1_sar_vh_desc_stddev',\n",
    "             's1_sar_vh_asc_mean',\n",
    "             's1_sar_vh_asc_stddev']\n",
    "\n",
    "to_extract = ['s1_sar_vv_desc_mean',\n",
    "             's1_sar_vv_desc_stddev',\n",
    "             's1_sar_vv_asc_mean',\n",
    "             's1_sar_vv_asc_stddev',\n",
    "             's1_sar_vh_desc_mean',\n",
    "             's1_sar_vh_desc_stddev',\n",
    "             's1_sar_vh_asc_mean',\n",
    "             's1_sar_vh_asc_stddev',\n",
    "             's1_sar_vdiv_desc_mean',\n",
    "             's1_sar_vdiv_desc_stddev',\n",
    "             's1_sar_vdiv_asc_mean',\n",
    "             's1_sar_vdiv_asc_stddev']\n",
    "\n",
    "tasks_all = []\n",
    "\n",
    "# Loop over satellites ------------------------------\n",
    "for name in to_extract:\n",
    "    print(name)\n",
    "        \n",
    "    sat = name\n",
    "    \n",
    "    if name in ['NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4', 'l8', 'l7', 'l5', 'l7_sdtime', 'l8_sdtime', 'l7_sdspace', 'l8_sdspace', 'aod', 'GlobalHumanModification', 'elevation', 'slope']:\n",
    "        if SURVEY_NAME == \"DHS\":\n",
    "            buffer_u = 2500\n",
    "            buffer_r = 2500\n",
    "            \n",
    "        if SURVEY_NAME == \"PAK_POINTS\":\n",
    "            buffer_u = 1500\n",
    "            buffer_r = 1500\n",
    "            \n",
    "        if SURVEY_NAME == \"PAK_CITY_POINTS\":\n",
    "            buffer_u = 750\n",
    "            buffer_r = 750\n",
    "            \n",
    "    if 's1_sar' in name:\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "                    \n",
    "    if name in ['ecmwf_weather',\n",
    "                'ecmwf_weather_q1', 'ecmwf_weather_q2', 'ecmwf_weather_q3', 'ecmwf_weather_q4']:\n",
    "        # 27km radius\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "        \n",
    "    if name == 'viirs181920_750':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 750\n",
    "        buffer_r = 750\n",
    "        \n",
    "    if name == 'viirs181920_1120':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1120\n",
    "        buffer_r = 1120\n",
    "                \n",
    "    if name == 'viirs181920_1250':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1250\n",
    "        buffer_r = 1250\n",
    "        \n",
    "    if name == 'viirs181920_1500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1500\n",
    "        buffer_r = 1500\n",
    "        \n",
    "    if name == 'viirs181920_2000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'viirs181920_2500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'viirs181920_3360':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 3360\n",
    "        buffer_r = 3360\n",
    "        \n",
    "    if name == 'viirs181920_5000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "\n",
    "    if name == 'viirs_750':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 750\n",
    "        buffer_r = 750\n",
    "            \n",
    "    if name == 'viirs_1120':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1120\n",
    "        buffer_r = 1120\n",
    "            \n",
    "    if name == 'viirs_1250':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1250\n",
    "        buffer_r = 1250\n",
    "        \n",
    "    if name == 'viirs_1500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 1500\n",
    "        buffer_r = 1500\n",
    "            \n",
    "    if name == 'viirs_2000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'viirs_2500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'viirs_3360':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 3360\n",
    "        buffer_r = 3360\n",
    "        \n",
    "    if name == 'viirs_5000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'viirs_sdtime_2500':\n",
    "        sat = 'viirs_sdtime'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'viirs_sdspace_2500':\n",
    "        sat = 'viirs_sdspace'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'worldpop_750':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 750\n",
    "        buffer_r = 750\n",
    "        \n",
    "    if name == 'worldpop_1500':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 1500\n",
    "        buffer_r = 1500\n",
    "        \n",
    "    if name == 'worldpop_2000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'worldpop_2500':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'worldpop_5000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'worldpop_10000':\n",
    "        sat = 'worldpop'\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "       \n",
    "    if name == 'worldpop2020_750':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 750\n",
    "        buffer_r = 750\n",
    "    \n",
    "    if name == 'worldpop2020_1500':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 1500\n",
    "        buffer_r = 1500\n",
    "        \n",
    "    if name == 'worldpop2020_2000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if name == 'worldpop2020_2500':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if name == 'worldpop2020_5000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if name == 'worldpop2020_10000':\n",
    "        sat = 'worldpop2020'\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "        \n",
    "    survey_df_use = survey_df.copy()\n",
    "    \n",
    "    # Define Chunk Size ---------------------------------\n",
    "    CHUNK_SIZE = 5000\n",
    "    \n",
    "    if sat in ['NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4']:\n",
    "        CHUNK_SIZE = 1\n",
    "        \n",
    "    if sat in ['NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4']:\n",
    "        survey_df_use = survey_df_use[survey_df_use.most_recent_survey == True]\n",
    "        \n",
    "    ## Check to see if file exists    \n",
    "    if (how_check_processed == 'file'):\n",
    "        \n",
    "        survey_df_use['chunk_id'] = chunk_ids(survey_df_use.shape[0], CHUNK_SIZE)\n",
    "\n",
    "        # Loop over satellites ------------------------------\n",
    "\n",
    "        # LOOP OVER YEARS AND CHUNKS TO EXTRACT DATA \n",
    "        for year_i in survey_df_use['year'].unique():\n",
    "\n",
    "            survey_df_year = survey_df_use[survey_df_use['year'] == year_i]\n",
    "\n",
    "            # LOOP OVER CHUNKS\n",
    "            for chunk_id_i in survey_df_year['chunk_id'].unique():\n",
    "                survey_df_year_i = survey_df_year[survey_df_year['chunk_id'] == chunk_id_i]\n",
    "\n",
    "                # TODO: CHANGE BACK - changes to \"gee_small_\" for pollution stuff due to issues.\n",
    "                file_name_i = 'gee_' + name + '_ubuff' + str(buffer_u) + '_rbuff' + str(buffer_r) + '_' + str(year_i) + '_' + str(chunk_id_i)    \n",
    "                file_name_i_csv = file_name_i + '.csv'\n",
    "\n",
    "                # ONLY EXTRACT DATA IF NOT ALREADY EXTRACTED\n",
    "\n",
    "                ## Check against file name\n",
    "                if (file_name_i_csv not in files_extracted):\n",
    "\n",
    "                    task_i = extract_sat(survey_df = survey_df_year_i, \n",
    "                                         buffer_size_urban = buffer_u, \n",
    "                                         buffer_size_rural = buffer_r, \n",
    "                                         year = year_i,\n",
    "                                         satellite = sat, \n",
    "                                         survey_name = SURVEY_NAME,\n",
    "                                         file_name = file_name_i)\n",
    "\n",
    "                    tasks_all.append(task_i)\n",
    "                    \n",
    "    if (how_check_processed == 'data'):\n",
    "        \n",
    "        survey_df_use_copy = survey_df_use.copy()\n",
    "\n",
    "        ## Root name\n",
    "        fname_root = 'gee_' + sat + '_ubuff' + str(buffer_u) + '_rbuff' + str(buffer_r)\n",
    "\n",
    "        ## Make dataframe\n",
    "        all_filenames = [i for i in glob.glob('*.{}'.format('.csv'))]\n",
    "        sat_files     = glob.glob(OUT_PATH + '/' + fname_root + '*')\n",
    "        processed_df  = pd.concat([pd.read_csv(f) for f in sat_files])\n",
    "\n",
    "        ## Merge and subset to not processed\n",
    "        processed_df = processed_df[['uid', 'year']]\n",
    "        processed_df['already_scraped'] = 1\n",
    "\n",
    "        survey_df_use_copy = survey_df_use_copy.merge(processed_df, on=['uid', 'year'], how='left')\n",
    "\n",
    "        survey_df_ntprcsd = survey_df_use_copy[survey_df_use_copy.already_scraped.isnull()]\n",
    "        \n",
    "        print(survey_df_ntprcsd.shape[0])\n",
    "\n",
    "        if (survey_df_ntprcsd.shape[0] > 0):\n",
    "        \n",
    "            ## Add chunks\n",
    "            survey_df_ntprcsd['chunk_id'] = chunk_ids(survey_df_ntprcsd.shape[0], CHUNK_SIZE)\n",
    "\n",
    "            # Loop over satellites ------------------------------\n",
    "\n",
    "            # LOOP OVER YEARS AND CHUNKS TO EXTRACT DATA \n",
    "            for year_i in survey_df_ntprcsd['year'].unique():\n",
    "\n",
    "                survey_df_year = survey_df_ntprcsd[survey_df_ntprcsd['year'] == year_i]\n",
    "\n",
    "                # LOOP OVER CHUNKS\n",
    "                for chunk_id_i in survey_df_year['chunk_id'].unique():\n",
    "                    survey_df_year_i = survey_df_year[survey_df_year['chunk_id'] == chunk_id_i]\n",
    "\n",
    "                    # TODO: CHANGE BACK - changes to \"gee_small_\" for pollution stuff due to issues.\n",
    "                    now = datetime.now()\n",
    "                    dt_string = now.strftime(\"%d%m%Y%H%M%S\")\n",
    "\n",
    "                    file_name_i = 'gee_' + name + '_ubuff' + str(buffer_u) + '_rbuff' + str(buffer_r) + '_' + str(year_i) + '_' + str(chunk_id_i) + '_' + str(dt_string)    \n",
    "                    file_name_i_csv = file_name_i + '.csv'\n",
    "\n",
    "                    # Extract data\n",
    "                    \n",
    "                    task_i = extract_sat(survey_df = survey_df_year_i, \n",
    "                                         buffer_size_urban = buffer_u, \n",
    "                                         buffer_size_rural = buffer_r, \n",
    "                                         year = year_i,\n",
    "                                         satellite = sat, \n",
    "                                         survey_name = SURVEY_NAME,\n",
    "                                         file_name = file_name_i)\n",
    "\n",
    "                    tasks_all.append(task_i)\n",
    "                    \n",
    "                    time.sleep(1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_extract = ['CH4', 'SO2', 'ozone', 'HCHO', 'CO', 'uv_aer', 'NO2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'CH4' in to_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>year</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>most_recent_survey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3540</th>\n",
       "      <td>BF199900000002</td>\n",
       "      <td>1998</td>\n",
       "      <td>12.515376</td>\n",
       "      <td>-1.690597</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76814</th>\n",
       "      <td>TG199800000137</td>\n",
       "      <td>1998</td>\n",
       "      <td>6.235644</td>\n",
       "      <td>1.472263</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76815</th>\n",
       "      <td>TG199800000138</td>\n",
       "      <td>1998</td>\n",
       "      <td>6.770265</td>\n",
       "      <td>1.512131</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76816</th>\n",
       "      <td>TG199800000139</td>\n",
       "      <td>1998</td>\n",
       "      <td>6.708370</td>\n",
       "      <td>1.491649</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19776</th>\n",
       "      <td>GH199800000130</td>\n",
       "      <td>1998</td>\n",
       "      <td>5.653637</td>\n",
       "      <td>0.024108</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55944</th>\n",
       "      <td>KE202000002154</td>\n",
       "      <td>2020</td>\n",
       "      <td>-3.998680</td>\n",
       "      <td>39.624774</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55943</th>\n",
       "      <td>KE202000002144</td>\n",
       "      <td>2020</td>\n",
       "      <td>-4.034569</td>\n",
       "      <td>39.686857</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55942</th>\n",
       "      <td>KE202000002140</td>\n",
       "      <td>2020</td>\n",
       "      <td>-4.012838</td>\n",
       "      <td>39.695890</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55940</th>\n",
       "      <td>KE202000002121</td>\n",
       "      <td>2020</td>\n",
       "      <td>-1.306749</td>\n",
       "      <td>36.893506</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56144</th>\n",
       "      <td>KE202000009183</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.623640</td>\n",
       "      <td>34.802932</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82227 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  uid  year   latitude  longitude  most_recent_survey\n",
       "3540   BF199900000002  1998  12.515376  -1.690597               False\n",
       "76814  TG199800000137  1998   6.235644   1.472263               False\n",
       "76815  TG199800000138  1998   6.770265   1.512131               False\n",
       "76816  TG199800000139  1998   6.708370   1.491649               False\n",
       "19776  GH199800000130  1998   5.653637   0.024108               False\n",
       "...               ...   ...        ...        ...                 ...\n",
       "55944  KE202000002154  2020  -3.998680  39.624774                True\n",
       "55943  KE202000002144  2020  -4.034569  39.686857                True\n",
       "55942  KE202000002140  2020  -4.012838  39.695890                True\n",
       "55940  KE202000002121  2020  -1.306749  36.893506                True\n",
       "56144  KE202000009183  2020   0.623640  34.802932                True\n",
       "\n",
       "[82227 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#survey_fc1 = survey_df[survey_df.uid.isin(['TG199800000508', 'TG199800000509', 'TG199800000510', 'TG199800000511'])]\n",
    "#survey_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#survey_fc = survey_to_fc_buffer(survey_fc1, 2500, 2500)\n",
    "survey_fc = survey_to_fc_buffer(survey_df.head(), 2500, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 100 # ok to upscale\n",
    "#SCALE = 2000\n",
    "\n",
    "# Year\n",
    "# Starts in late 2014\n",
    "#if year < 2017:\n",
    "#    year_use = 2017\n",
    "#else:\n",
    "#    year_use = year\n",
    "\n",
    "year_use = 2018\n",
    "\n",
    "year_plus = year_use + 0\n",
    "year_minus = year_use - 0\n",
    "\n",
    "year_minus_str = str(year_minus) + '-01-01'\n",
    "year_plus_str = str(year_plus) + '-12-31'\n",
    "\n",
    "image = ee.ImageCollection('COPERNICUS/S1_GRD')\\\n",
    "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'HV'))\\\n",
    "    .filter(ee.Filter.eq('resolution_meters', 10))\\\n",
    "    .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\\\n",
    "    .select('HV')\\\n",
    "    .filterDate(year_minus_str, year_plus_str)\\\n",
    "    .reduce(ee.Reducer.stdDev())\n",
    "\n",
    "\n",
    "#.reduce(ee.Reducer.stdDev())\n",
    "#BANDS = ['VV', 'VH']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = image.reduceRegions(collection = survey_fc,\n",
    "                           reducer = ee.Reducer.mean(),\n",
    "                           scale = SCALE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_to_export = BANDS.copy()\n",
    "bands_to_export.append('uid')\n",
    "bands_to_export.append('year')\n",
    "#print(bands_to_export)\n",
    "\n",
    "bands_to_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ee.batch.Export.table.toDrive(collection=vals, \n",
    "                                     folder='satellite_data_from_gee_' + 'dhs'.lower(), \n",
    "                                     description='TEST', \n",
    "                                     fileFormat='CSV',\n",
    "                                     selectors = bands_to_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "EEException",
     "evalue": "Collection query aborted after accumulating over 5000 elements.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/ee/data.py:330\u001b[0m, in \u001b[0;36m_execute_cloud_call\u001b[0;34m(call, num_retries)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m googleapiclient\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mHttpError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/googleapiclient/_helpers.py:134\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/googleapiclient/http.py:915\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/value:compute?prettyPrint=false&alt=json returned \"Collection query aborted after accumulating over 5000 elements.\". Details: \"Collection query aborted after accumulating over 5000 elements.\">",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEEException\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m s1_vh_list \u001b[38;5;241m=\u001b[39m s1_vh_values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Print the list of point features\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43ms1_vv_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(s1_vh_list\u001b[38;5;241m.\u001b[39mgetInfo())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/ee/collection.py:133\u001b[0m, in \u001b[0;36mCollection.getInfo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetInfo\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    121\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns all the known information about this collection.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m  This function makes an REST call to to retrieve all the known information\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m         properties.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/ee/computedobject.py:98\u001b[0m, in \u001b[0;36mComputedObject.getInfo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetInfo\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     93\u001b[0m   \u001b[38;5;124;03m\"\"\"Fetch and return information about this object.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    The object can evaluate to anything.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputeValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/ee/data.py:735\u001b[0m, in \u001b[0;36mcomputeValue\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomputeValue\u001b[39m(obj):\n\u001b[1;32m    727\u001b[0m   \u001b[38;5;124;03m\"\"\"Sends a request to compute a value.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \n\u001b[1;32m    729\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;124;03m    The result of evaluating that object on the server.\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 735\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_execute_cloud_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_get_cloud_api_resource\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexpression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfor_cloud_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m          \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get_projects_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m          \u001b[49m\u001b[43mprettyPrint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/ee/data.py:332\u001b[0m, in \u001b[0;36m_execute_cloud_call\u001b[0;34m(call, num_retries)\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mexecute(num_retries\u001b[38;5;241m=\u001b[39mnum_retries)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m googleapiclient\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mHttpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 332\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m _translate_cloud_exception(e)\n",
      "\u001b[0;31mEEException\u001b[0m: Collection query aborted after accumulating over 5000 elements."
     ]
    }
   ],
   "source": [
    "import ee\n",
    "\n",
    "# Initialize the Earth Engine API\n",
    "ee.Initialize()\n",
    "\n",
    "# Define the study area using a geometry\n",
    "study_area = ee.Geometry.Polygon([\n",
    "    [[-122.49443054199219, 37.73132798754766],\n",
    "     [-122.49443054199219, 37.62568842838177],\n",
    "     [-122.3291015625, 37.62568842838177],\n",
    "     [-122.3291015625, 37.73132798754766]]\n",
    "])\n",
    "\n",
    "# Create a list of points within the study area\n",
    "points = ee.List.sequence(0, 10).map(lambda i: ee.Feature(study_area.centroid(10), {'name': i}))\n",
    "\n",
    "# Convert the list of points to a FeatureCollection\n",
    "points_fc = ee.FeatureCollection(points)\n",
    "\n",
    "# Load Sentinel-1 VV and VH images\n",
    "s1_vv = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "         .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "         .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
    "         .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))\n",
    "         .select('VV'))\n",
    "\n",
    "s1_vh = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "         .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "         .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "         .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))\n",
    "         .select('VH'))\n",
    "\n",
    "# Extract values at the points for each image in the collections\n",
    "s1_vv_values = s1_vv.map(lambda image: image.reduceRegions(\n",
    "    collection=points_fc,\n",
    "    reducer=ee.Reducer.mean(),\n",
    "    scale=10\n",
    "))\n",
    "\n",
    "s1_vh_values = s1_vh.map(lambda image: image.reduceRegions(\n",
    "    collection=points_fc,\n",
    "    reducer=ee.Reducer.mean(),\n",
    "    scale=10\n",
    "))\n",
    "\n",
    "# Flatten the collections of point features to a single list\n",
    "s1_vv_list = s1_vv_values.flatten()\n",
    "s1_vh_list = s1_vh_values.flatten()\n",
    "\n",
    "# Print the list of point features\n",
    "print(s1_vv_list.getInfo())\n",
    "print(s1_vh_list.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'hh' in 's1_sar_hh_asc_med':\n",
    "    VAR = 'HH'\n",
    "    \n",
    "if 'HV' in 's1_sar_hh_asc_med':\n",
    "    VAR = 'HV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
