{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=oxumYu0w2wllNuFBkA4tlM7a09NQDkfai4rOZDKk4D0&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=oxumYu0w2wllNuFBkA4tlM7a09NQDkfai4rOZDKk4D0&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1AX4XfWi9zhcHbKWOdOiIXN_mRy-FkHaRG5YFq4TQykPHVtrHFt_gGlyhdjw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geetools\n",
    "from geetools import ui, cloud_mask\n",
    "import os, datetime\n",
    "import config as cf\n",
    "import pandas as pd\n",
    "import eeconvert\n",
    "import time\n",
    "import geopandas as gpd\n",
    "\n",
    "cloud_mask_landsatSR = cloud_mask.landsatSR()\n",
    "cloud_mask_sentinel2 = cloud_mask.sentinel2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURVEY_NAME = 'DHS'\n",
    "REEXTRACT_IF_FILE_EXISTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gis.stackexchange.com/questions/257727/iterate-over-imagecollection-returning-pandas-dataframe-using-earth-engine-pyt\n",
    "def fc2df(fc):\n",
    "    # Convert a FeatureCollection into a pandas DataFrame\n",
    "    # Features is a list of dict with the output\n",
    "    features = fc.getInfo()['features']\n",
    "\n",
    "    dictarr = []\n",
    "\n",
    "    for f in features:\n",
    "        # Store all attributes in a dict\n",
    "        attr = f['properties']\n",
    "        # and treat geometry separately\n",
    "        attr['geometry'] = f['geometry']  # GeoJSON Feature!\n",
    "        # attr['geometrytype'] = f['geometry']['type']\n",
    "        dictarr.append(attr)\n",
    "\n",
    "    df = gpd.GeoDataFrame(dictarr)\n",
    "    # Convert GeoJSON features to shape\n",
    "    df = df.drop(columns=['geometry'])\n",
    "    return df\n",
    "\n",
    "def survey_to_fc_buffer(survey_df, buffer_size_urban, buffer_size_rural):\n",
    "    '''\n",
    "    Convert pandas dataframe of survey locations to a feature collection. \n",
    "    \n",
    "    Inputs:\n",
    "        survey_df: pandas dataframe of survey locations. Function assumes \n",
    "                   the dataframe contains (1) latitude, (2) longitude and\n",
    "                   (3) uid variables. Assumes coordinates in WGS84.\n",
    "    Returns:\n",
    "        (feature collection)\n",
    "    '''\n",
    "    \n",
    "    survey_fc_list = []\n",
    "    \n",
    "    n_rows = survey_df.shape[0]\n",
    "    for i in range(0, n_rows):\n",
    "        survey_df_i = survey_df.iloc[[i]]\n",
    "        \n",
    "        ur = survey_df_i['urban_rural'].iloc[0]\n",
    "        if ur == 'U':\n",
    "            buffer_size = buffer_size_urban\n",
    "        elif ur == 'R':\n",
    "            buffer_size = buffer_size_rural\n",
    "\n",
    "        f_i = ee.Feature(ee.Geometry.Point([survey_df_i['longitude'].iloc[0], \n",
    "                                            survey_df_i['latitude'].iloc[0]]), \n",
    "                         {'uid': survey_df_i['uid'].iloc[0]})\n",
    "        \n",
    "        f_i = f_i.buffer(buffer_size)\n",
    "\n",
    "        survey_fc_list.append(f_i)\n",
    "        \n",
    "    survey_fc = ee.FeatureCollection(survey_fc_list)\n",
    "    \n",
    "    return survey_fc\n",
    "\n",
    "def extract_sat(survey_df, buffer_size_urban, buffer_size_rural, satellite, year, chunk):\n",
    "    '''\n",
    "    Extract satellite imagery to locations \n",
    "    \n",
    "    Inputs:\n",
    "        survey_df: pandas dataframe of survey locations. Function assumes \n",
    "                   the dataframe contains (1) latitude, (2) longitude and\n",
    "                   (3) uid variables. Assumes coordinates in WGS84.\n",
    "    Returns:\n",
    "        (feature collection)\n",
    "    '''\n",
    "    \n",
    "    #print(survey_df.uid)\n",
    "    \n",
    "    year_start_sp5 = \"2018-01-01\"\n",
    "    year_end_sp5 = '2020-12-31'\n",
    "    \n",
    "    # Prep l7 ---------------------------------------------------\n",
    "    if satellite == 'worldpop':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 100 \n",
    "        \n",
    "        # Year\n",
    "        year_use = year\n",
    "        \n",
    "        year_plus = year_use\n",
    "        year_minus = year_use\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('WorldPop/GP/100m/pop')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        # After the reducer computers the sum, it names the value \"sum\", not population\n",
    "        BANDS = ['sum']\n",
    "    \n",
    "    # Prep l7 ---------------------------------------------------\n",
    "    if satellite == 'l7':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 100 # ok to upscale\n",
    "        \n",
    "        # Year\n",
    "        year_use = year\n",
    "        \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC07/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        ndvi = image.normalizedDifference(['B4', 'B3']).rename('NDVI');\n",
    "        image = image.addBands(ndvi)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
    "        \n",
    "    # Sentinel-5P OFFL AER AI: Offline UV Aerosol Index  -------------------\n",
    "    if satellite == 'uv_aer':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_AER_AI\")\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['absorbing_aerosol_index']\n",
    "        \n",
    "    # Sentinel-5P OFFL CO: Offline Carbon Monoxide  -------------------\n",
    "    if satellite == 'CO':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CO\")\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['CO_column_number_density', 'H2O_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL HCHO: Offline Formaldehyde  -------------------\n",
    "    if satellite == 'HCHO':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_HCHO\")\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['tropospheric_HCHO_column_number_density', 'tropospheric_HCHO_column_number_density_amf']\n",
    "        \n",
    "    # Sentinel-5P Nitrogen Dioxide  -----------------------------\n",
    "    if satellite == 'NO2':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_NO2\")\\\n",
    "            .filterDate(year_start_sp5, year_end_sp5)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['NO2_column_number_density', 'tropospheric_NO2_column_number_density',\\\n",
    "                 'stratospheric_NO2_column_number_density', 'NO2_slant_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL O3: Offline Ozone  -------------------\n",
    "    if satellite == 'ozone':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_O3\")\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['O3_column_number_density', 'O3_effective_temperature']\n",
    "        \n",
    "    # Sentinel-5P OFFL SO2: Offline Sulphur Dioxide  -------------------\n",
    "    if satellite == 'SO2':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_SO2\")\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['SO2_column_number_density', 'SO2_column_number_density_amf', 'SO2_slant_column_number_density']\n",
    "        \n",
    "    # Sentinel-5P OFFL CH4: Offline Methane  -------------------\n",
    "    if satellite == 'CH4':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "        \n",
    "        # Starts in 2018; take all years\n",
    "        image = ee.ImageCollection(\"COPERNICUS/S5P/OFFL/L3_CH4\")\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['CH4_column_volume_mixing_ratio_dry_air']\n",
    "        \n",
    "    # CSP gHM: Global Human Modification ---------------------------------\n",
    "    if satellite == 'GlobalHumanModification':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.ImageCollection(\"CSP/HM/GlobalHumanModification\")\\\n",
    "            .median()\n",
    "        \n",
    "        # Original name is \"gHM\", but because only one value, it takes the\n",
    "        # name of the reducer; we use mean\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # WorldClim BIO Variables V1 ---------------------------------\n",
    "    if satellite == 'worldclim_bio':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.Image('WORLDCLIM/V1/BIO')\n",
    "        \n",
    "        BANDS = ['bio01', 'bio02', 'bio03', 'bio04', 'bio05', 'bio06', 'bio07', 'bio08', 'bio09', 'bio10',\\\n",
    "                 'bio11', 'bio12', 'bio13', 'bio14', 'bio15', 'bio16', 'bio17', 'bio18', 'bio19']\n",
    "        \n",
    "    # Elevation - SRTM ------------------------------------------\n",
    "    if satellite == 'elevation':\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 1000 # ok to upscale\n",
    "                \n",
    "        image = ee.Image('USGS/SRTMGL1_003') # CGIAR/SRTM90_V4\n",
    "        \n",
    "        # elevation?\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # Elevation - SRTM ------------------------------------------\n",
    "    if satellite == 'slope':\n",
    "        # https://developers.google.com/earth-engine/datasets/catalog/CGIAR_SRTM90_V4#description\n",
    "        \n",
    "        # Scale\n",
    "        SCALE = 500 # ok to upscale\n",
    "                \n",
    "        image_raw = ee.Image('USGS/SRTMGL1_003') # CGIAR/SRTM90_V4\n",
    "        image_elev = image_raw.select('elevation')\n",
    "        image = ee.Terrain.slope(image_elev)\n",
    "                \n",
    "        # mean?\n",
    "        BANDS = ['mean']\n",
    "        \n",
    "    # Prep l8 ---------------------------------------------------\n",
    "    if satellite == 'l8':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        #SCALE = 2000\n",
    "        \n",
    "        # Year\n",
    "        # landsat 8 starts in April 2013; if year is less than\n",
    "        # 2014, use 2014 as year (to ensure have year before and after)\n",
    "        if year < 2014:\n",
    "            year_use = 2014\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_landsatSR)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "\n",
    "        # https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri\n",
    "        ndvi = image.normalizedDifference(['B5', 'B4']).rename('NDVI');\n",
    "        ndbi = image.normalizedDifference(['B6', 'B5']).rename('NDBI');\n",
    "        image = image.addBands(ndvi)\n",
    "        image = image.addBands(ndbi)\n",
    "        \n",
    "        bu = image.select('NDBI').subtract(image.select('NDVI')).rename('BU')\n",
    "        image = image.addBands(bu)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11', 'NDVI', 'NDBI', 'BU']\n",
    "        #BANDS = ['NDVI']\n",
    "        \n",
    "    # Prep s2 ---------------------------------------------------\n",
    "    if satellite == 's2':\n",
    "        \n",
    "        SCALE = 100 # ok to upscale\n",
    "        \n",
    "        # Year\n",
    "        # sentinel starts in March 2017; juse use 2018\n",
    "        year_use = 2018\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-12-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('COPERNICUS/S2_SR')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .map(cloud_mask_sentinel2)\\\n",
    "            .median()\\\n",
    "            .multiply(0.0001)\n",
    "        \n",
    "        ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI');\n",
    "        image = image.addBands(ndvi)\n",
    "        \n",
    "        BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'AOT', 'NDVI']\n",
    "\n",
    "        image = image.select(BANDS) \n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'gridmet_drought':\n",
    "        \n",
    "        SCALE = 5000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"GRIDMET/DROUGHT\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['pdsi', 'z', 'eddi1y', 'eddi2y', 'eddi5y']\n",
    "    \n",
    "    \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q1':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-01-01'\n",
    "        year_plus_str = str(year) + '-03-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q2':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-04-01'\n",
    "        year_plus_str = str(year) + '-06-30'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q3':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-07-01'\n",
    "        year_plus_str = str(year) + '-09-30'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "        \n",
    "    # Prep drought ---------------------------------------------------\n",
    "    if satellite == 'ecmwf_weather_q4':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "\n",
    "        year_minus_str = str(year) + '-10-01'\n",
    "        year_plus_str = str(year) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection(\"ECMWF/ERA5/DAILY\")\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .mean()\n",
    "        \n",
    "        BANDS = ['mean_2m_air_temperature', \n",
    "                 'minimum_2m_air_temperature', \n",
    "                 'maximum_2m_air_temperature',\n",
    "                 'total_precipitation']\n",
    "    \n",
    "    # Prep viirs ---------------------------------------------------\n",
    "    if satellite == 'viirs':\n",
    "        \n",
    "        SCALE = 500 \n",
    "        \n",
    "        # Year\n",
    "        # VIIRS starts in April 2012; if year is less than\n",
    "        # 2013, use 2013 as year (to ensure have year before and after)\n",
    "        if year < 2013:\n",
    "            year_use = 2013\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['avg_rad']\n",
    "        \n",
    "    # Prep DMSP ---------------------------------------------------\n",
    "    if satellite == 'dmsp':\n",
    "        \n",
    "        SCALE = 1000 \n",
    "        \n",
    "        # Year\n",
    "        # DMSP-OLS starts in 2013; if year is more than\n",
    "        # 2012, use 2012 as year (to ensure have year before and after)\n",
    "        if year > 2012:\n",
    "            year_use = 2012\n",
    "        else:\n",
    "            year_use = year\n",
    "                    \n",
    "        year_plus = year_use + 1\n",
    "        year_minus = year_use - 1\n",
    "        \n",
    "        year_minus_str = str(year_minus) + '-01-01'\n",
    "        year_plus_str = str(year_plus) + '-12-31'\n",
    "        \n",
    "        image = ee.ImageCollection('NOAA/DMSP-OLS/NIGHTTIME_LIGHTS')\\\n",
    "            .filterDate(year_minus_str, year_plus_str)\\\n",
    "            .median()\n",
    "        \n",
    "        BANDS = ['stable_lights', 'avg_lights_x_pct']\n",
    "    \n",
    "    # Prep Survey ---------------------------------------------------\n",
    "    survey_fc = survey_to_fc_buffer(survey_df, buffer_size_urban, buffer_size_rural)\n",
    "    \n",
    "    # Extract Values ---------------------------------------------------\n",
    "    if satellite == 'worldpop':\n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.sum(),\n",
    "                                   scale = SCALE,\n",
    "                                   tileScale = 8)\n",
    "    else:\n",
    "        vals = image.reduceRegions(collection = survey_fc,\n",
    "                                   reducer = ee.Reducer.mean(),\n",
    "                                   scale = SCALE,\n",
    "                                   tileScale = 8)\n",
    "\n",
    "    # OLD =============\n",
    "    # Survey dataset that only contains the uid variable\n",
    "    #survey_df = survey_df[['uid']]\n",
    "            \n",
    "    #for band_i in BANDS:\n",
    "    #    survey_df[satellite + '_' + band_i] = vals.aggregate_array(band_i).getInfo()\n",
    "        \n",
    "    # NEW =============\n",
    "    #df_out = fc2df(vals)\n",
    "    #print(df_out)\n",
    "    #df_out = pd.DataFrame()\n",
    "    \n",
    "    bands_to_export = BANDS.copy()\n",
    "    bands_to_export.append('uid')\n",
    "    #print(bands_to_export)\n",
    "    \n",
    "    task = ee.batch.Export.table.toDrive(collection=vals, \n",
    "                                         folder='satellite_data_from_gee_dhs', \n",
    "                                         description=satellite + \"_ubuff\" + str(buffer_size_urban) + '_rbuff' + str(buffer_size_rural) + \"_\" + str(year) + '_' + str(chunk), \n",
    "                                         fileFormat='CSV',\n",
    "                                         selectors = bands_to_export)\n",
    "    # selectors=props\n",
    "    task.start()\n",
    "    #ee.batch.data.startProcessing(mytask.id, mytask.config)\n",
    "    \n",
    "    if False:\n",
    "        time_elapsed = 0\n",
    "        while task.active():\n",
    "            if((time_elapsed % 60) == 0):\n",
    "                print('Polling for task (id: {}).'.format(task.id))\n",
    "            time.sleep(5)\n",
    "            time_elapsed = time_elapsed + 5\n",
    "        \n",
    "    return task\n",
    "\n",
    "def extract_satellite_in_chunks(survey_df, buffer_size_urban, buffer_size_rural, satellite, year):\n",
    "    \n",
    "    vals_df_list = []\n",
    "    \n",
    "    for chunk_i in list(np.unique(survey_df.chunk_id)):\n",
    "        #print(chunk_i)\n",
    "        #time.sleep(5)\n",
    "\n",
    "        survey_df_i = survey_df[survey_df['chunk_id'] == chunk_i]\n",
    "        #print(survey_df_i.shape)\n",
    "        vals_i_df = extract_sat(survey_df_i, buffer_size_urban, buffer_size_rural, satellite, year, chunk_i)\n",
    "\n",
    "        vals_df_list.append(vals_i_df)\n",
    "\n",
    "    #vals_df = pd.concat(vals_df_list)\n",
    "    \n",
    "    return vals_df_list\n",
    "\n",
    "def extract_satellite_by_year(survey_df, buffer_size_urban, buffer_size_rural, satellite):\n",
    "    \n",
    "    vals_df_list = []\n",
    "    \n",
    "    for year_i in list(np.unique(survey_df.year)):\n",
    "        #print(year_i)\n",
    "        #time.sleep(5)\n",
    "\n",
    "        survey_df_i = survey_df[survey_df['year'] == year_i]\n",
    "        vals_i_df = extract_satellite_in_chunks(survey_df_i, buffer_size_urban, buffer_size_rural, satellite, year_i)\n",
    "\n",
    "        vals_df_list.append(vals_i_df)\n",
    "\n",
    "    #vals_df = pd.concat(vals_df_list)\n",
    "    \n",
    "    return vals_df_list\n",
    "\n",
    "def chunk_ids(total_length, chunk_size):\n",
    "    n_numbers = np.ceil(total_length / chunk_size)\n",
    "    n_numbers = int(n_numbers)\n",
    "    \n",
    "    chunk_ids = list(range(0,n_numbers)) * chunk_size\n",
    "    chunk_ids.sort()\n",
    "    chunk_ids = chunk_ids[:total_length]\n",
    "    \n",
    "    return chunk_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/Prep Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = pd.read_csv(os.path.join(cf.SECURE_DATA_DIRECTORY, 'Data', SURVEY_NAME, 'FinalData - PII', 'GPS_uid_crosswalk.csv'))\n",
    "survey_df = survey_df.sort_values('year')\n",
    "#survey_df = survey_df[survey_df.uid != 'IA201400180012']\n",
    "\n",
    "CHUNK_SIZE = 5000\n",
    "survey_years = list(survey_df.year.unique())\n",
    "survey_df['chunk_id'] = chunk_ids(survey_df.shape[0], CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df_i = survey_df[survey_df.year == 2010]\n",
    "survey_df_i = survey_df_i[survey_df_i.chunk_id == 1]\n",
    "survey_df_i = survey_df_i.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_fc_i = survey_to_fc_buffer(survey_df_i, 10000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2574\n",
      "2454\n"
     ]
    }
   ],
   "source": [
    "print(len(a_uid))\n",
    "print(len(a_bio01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gis.stackexchange.com/questions/257727/iterate-over-imagecollection-returning-pandas-dataframe-using-earth-engine-pyt\n",
    "def fc2df(fc):\n",
    "    # Convert a FeatureCollection into a pandas DataFrame\n",
    "    # Features is a list of dict with the output\n",
    "    features = fc.getInfo()['features']\n",
    "\n",
    "    dictarr = []\n",
    "\n",
    "    for f in features:\n",
    "        # Store all attributes in a dict\n",
    "        attr = f['properties']\n",
    "        # and treat geometry separately\n",
    "        attr['geometry'] = f['geometry']  # GeoJSON Feature!\n",
    "        # attr['geometrytype'] = f['geometry']['type']\n",
    "        dictarr.append(attr)\n",
    "\n",
    "    df = gpd.GeoDataFrame(dictarr)\n",
    "    # Convert GeoJSON features to shape\n",
    "    df = df.drop(columns=['geometry'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bio01</th>\n",
       "      <th>bio02</th>\n",
       "      <th>bio03</th>\n",
       "      <th>bio04</th>\n",
       "      <th>bio05</th>\n",
       "      <th>bio06</th>\n",
       "      <th>bio07</th>\n",
       "      <th>bio08</th>\n",
       "      <th>bio09</th>\n",
       "      <th>bio10</th>\n",
       "      <th>bio11</th>\n",
       "      <th>bio12</th>\n",
       "      <th>bio13</th>\n",
       "      <th>bio14</th>\n",
       "      <th>bio15</th>\n",
       "      <th>bio16</th>\n",
       "      <th>bio17</th>\n",
       "      <th>bio18</th>\n",
       "      <th>bio19</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>259.441360</td>\n",
       "      <td>95.948508</td>\n",
       "      <td>84.245615</td>\n",
       "      <td>383.436229</td>\n",
       "      <td>313.885636</td>\n",
       "      <td>201.002703</td>\n",
       "      <td>112.882933</td>\n",
       "      <td>260.791517</td>\n",
       "      <td>253.903253</td>\n",
       "      <td>262.767437</td>\n",
       "      <td>253.585260</td>\n",
       "      <td>2820.033976</td>\n",
       "      <td>319.113157</td>\n",
       "      <td>130.495857</td>\n",
       "      <td>25.729098</td>\n",
       "      <td>907.313554</td>\n",
       "      <td>453.955298</td>\n",
       "      <td>721.140015</td>\n",
       "      <td>547.723125</td>\n",
       "      <td>CO201000004201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259.379068</td>\n",
       "      <td>96.630702</td>\n",
       "      <td>84.635908</td>\n",
       "      <td>375.786338</td>\n",
       "      <td>314.028793</td>\n",
       "      <td>200.853644</td>\n",
       "      <td>113.175150</td>\n",
       "      <td>260.603392</td>\n",
       "      <td>253.973545</td>\n",
       "      <td>262.620191</td>\n",
       "      <td>253.625346</td>\n",
       "      <td>2797.763529</td>\n",
       "      <td>317.720213</td>\n",
       "      <td>130.091674</td>\n",
       "      <td>26.197656</td>\n",
       "      <td>903.347646</td>\n",
       "      <td>446.844742</td>\n",
       "      <td>712.946625</td>\n",
       "      <td>541.175439</td>\n",
       "      <td>CO201000004203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259.387229</td>\n",
       "      <td>96.051129</td>\n",
       "      <td>84.314433</td>\n",
       "      <td>382.172891</td>\n",
       "      <td>313.860583</td>\n",
       "      <td>200.940873</td>\n",
       "      <td>112.919710</td>\n",
       "      <td>260.721807</td>\n",
       "      <td>253.862092</td>\n",
       "      <td>262.685453</td>\n",
       "      <td>253.545269</td>\n",
       "      <td>2817.273641</td>\n",
       "      <td>318.392661</td>\n",
       "      <td>130.218978</td>\n",
       "      <td>25.865060</td>\n",
       "      <td>906.723706</td>\n",
       "      <td>452.349605</td>\n",
       "      <td>721.071915</td>\n",
       "      <td>546.611677</td>\n",
       "      <td>CO201000004204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>259.407052</td>\n",
       "      <td>96.274818</td>\n",
       "      <td>84.422709</td>\n",
       "      <td>379.940554</td>\n",
       "      <td>313.950590</td>\n",
       "      <td>200.911668</td>\n",
       "      <td>113.038922</td>\n",
       "      <td>260.686096</td>\n",
       "      <td>253.921213</td>\n",
       "      <td>262.688083</td>\n",
       "      <td>253.592746</td>\n",
       "      <td>2809.602807</td>\n",
       "      <td>318.227797</td>\n",
       "      <td>130.228514</td>\n",
       "      <td>25.980193</td>\n",
       "      <td>905.383259</td>\n",
       "      <td>450.293707</td>\n",
       "      <td>718.551271</td>\n",
       "      <td>544.573983</td>\n",
       "      <td>CO201000004205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>259.410129</td>\n",
       "      <td>96.205329</td>\n",
       "      <td>84.384316</td>\n",
       "      <td>380.677006</td>\n",
       "      <td>313.933108</td>\n",
       "      <td>200.925036</td>\n",
       "      <td>113.008072</td>\n",
       "      <td>260.700783</td>\n",
       "      <td>253.911041</td>\n",
       "      <td>262.699576</td>\n",
       "      <td>253.585206</td>\n",
       "      <td>2811.925865</td>\n",
       "      <td>318.326715</td>\n",
       "      <td>130.250280</td>\n",
       "      <td>25.940061</td>\n",
       "      <td>905.795099</td>\n",
       "      <td>450.966076</td>\n",
       "      <td>719.314179</td>\n",
       "      <td>545.224981</td>\n",
       "      <td>CO201000004202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bio01      bio02      bio03       bio04       bio05       bio06  \\\n",
       "0  259.441360  95.948508  84.245615  383.436229  313.885636  201.002703   \n",
       "1  259.379068  96.630702  84.635908  375.786338  314.028793  200.853644   \n",
       "2  259.387229  96.051129  84.314433  382.172891  313.860583  200.940873   \n",
       "3  259.407052  96.274818  84.422709  379.940554  313.950590  200.911668   \n",
       "4  259.410129  96.205329  84.384316  380.677006  313.933108  200.925036   \n",
       "\n",
       "        bio07       bio08       bio09       bio10       bio11        bio12  \\\n",
       "0  112.882933  260.791517  253.903253  262.767437  253.585260  2820.033976   \n",
       "1  113.175150  260.603392  253.973545  262.620191  253.625346  2797.763529   \n",
       "2  112.919710  260.721807  253.862092  262.685453  253.545269  2817.273641   \n",
       "3  113.038922  260.686096  253.921213  262.688083  253.592746  2809.602807   \n",
       "4  113.008072  260.700783  253.911041  262.699576  253.585206  2811.925865   \n",
       "\n",
       "        bio13       bio14      bio15       bio16       bio17       bio18  \\\n",
       "0  319.113157  130.495857  25.729098  907.313554  453.955298  721.140015   \n",
       "1  317.720213  130.091674  26.197656  903.347646  446.844742  712.946625   \n",
       "2  318.392661  130.218978  25.865060  906.723706  452.349605  721.071915   \n",
       "3  318.227797  130.228514  25.980193  905.383259  450.293707  718.551271   \n",
       "4  318.326715  130.250280  25.940061  905.795099  450.966076  719.314179   \n",
       "\n",
       "        bio19             uid  \n",
       "0  547.723125  CO201000004201  \n",
       "1  541.175439  CO201000004203  \n",
       "2  546.611677  CO201000004204  \n",
       "3  544.573983  CO201000004205  \n",
       "4  545.224981  CO201000004202  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals_df = fc2df(vals)\n",
    "vals_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If re-extract, delete existing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REEXTRACT_IF_FILE_EXISTS:\n",
    "    print(\"Deleting existing files\")\n",
    "\n",
    "    ## Path with files\n",
    "    OUT_PATH = os.path.join(cf.GOOGLEDRIVE_DIRECTORY, \n",
    "                            'Data', \n",
    "                             SURVEY_NAME, \n",
    "                             'FinalData', \n",
    "                             'Individual Datasets',\n",
    "                             'satellite_data_from_gee_' + SURVEY_NAME.lower())\n",
    "\n",
    "    ## Grab csv files\n",
    "    files_to_rm = [x for x in os.listdir(OUT_PATH) if '.csv' in x]\n",
    "\n",
    "    ## Delete files\n",
    "    for file_i in files_to_rm:\n",
    "\n",
    "        path_i = os.path.join(OUT_PATH, file_i)\n",
    "        os.remove(path_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of files already extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['viirs_ubuff5000_rbuff5000',\n",
       " '.DS_Store',\n",
       " 'viirs_ubuff2000_rbuff2000',\n",
       " 'worldpop_ubuff10000_rbuff10000',\n",
       " 'NO2_ubuff2500_rbuff2500',\n",
       " 'uv_aer_ubuff2500_rbuff2500',\n",
       " 'HCHO_ubuff2500_rbuff2500',\n",
       " 'l8_ubuff2500_rbuff2500',\n",
       " 'CH4_ubuff2500_rbuff2500',\n",
       " 'gridmet_drought_ubuff5000_rbuff5000',\n",
       " 'elevation_ubuff5000_rbuff5000',\n",
       " 'SO2_ubuff2500_rbuff2500',\n",
       " 'slope_ubuff5000_rbuff5000',\n",
       " 'viirs_ubuff2500_rbuff2500',\n",
       " 'GlobalHumanModification_ubuff10000_rbuff10000',\n",
       " 'ozone_ubuff2500_rbuff2500',\n",
       " 'CO_ubuff2500_rbuff2500',\n",
       " 'NO2_ubuff5000_rbuff5000']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_DATA_PATH = os.path.join(cf.DROPBOX_DIRECTORY, 'Data', \n",
    "                            SURVEY_NAME, 'FinalData', \n",
    "                            'Individual Datasets', 'satellite_data_from_gee')\n",
    "\n",
    "\n",
    "files_already_extracted = [x.replace('.Rds', '') for x in os.listdir(DB_DATA_PATH)]\n",
    "files_already_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecmwf_weather\n",
      "ecmwf_weather_ubuff10000_rbuff10000\n",
      "ecmwf_weather_q1\n",
      "ecmwf_weather_q1_ubuff10000_rbuff10000\n",
      "ecmwf_weather_q2\n",
      "ecmwf_weather_q2_ubuff10000_rbuff10000\n",
      "ecmwf_weather_q3\n",
      "ecmwf_weather_q3_ubuff10000_rbuff10000\n",
      "ecmwf_weather_q4\n",
      "ecmwf_weather_q4_ubuff10000_rbuff10000\n"
     ]
    }
   ],
   "source": [
    "to_extract = ['elevation', \n",
    "            'slope',\n",
    "            'viirs_2000',\n",
    "            'viirs_2500',\n",
    "            'viirs_5000',\n",
    "            'GlobalHumanModification',\n",
    "            'worldpop',\n",
    "            'l8',\n",
    "            'gridmet_weather',\n",
    "              'ecmwf_weather',\n",
    "             'ecmwf_weather_q1',\n",
    "             'ecmwf_weather_q2',\n",
    "             'ecmwf_weather_q3',\n",
    "             'ecmwf_weather_q4',\n",
    "            'NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4']\n",
    "\n",
    "tasks_all = []\n",
    "for sat in to_extract:\n",
    "    print(sat)\n",
    "        \n",
    "    if sat in ['NO2', 'uv_aer', 'CO', 'HCHO', 'ozone', 'SO2', 'CH4', 'l8']:\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "    \n",
    "    if sat in ['elevation', 'slope', 'gridmet_drought']:\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if sat in ['worldclim_bio', 'GlobalHumanModification', 'ecmwf_weather',\n",
    "              'ecmwf_weather_q1', 'ecmwf_weather_q2', 'ecmwf_weather_q3', 'ecmwf_weather_q4']:\n",
    "        buffer_u = 10000\n",
    "        buffer_r = 10000\n",
    "                \n",
    "    if sat == 'viirs_2000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2000\n",
    "        buffer_r = 2000\n",
    "        \n",
    "    if sat == 'viirs_2500':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 2500\n",
    "        buffer_r = 2500\n",
    "        \n",
    "    if sat == 'viirs_5000':\n",
    "        sat = 'viirs'\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    if sat == 'worldclim_bio':\n",
    "        buffer_u = 5000\n",
    "        buffer_r = 5000\n",
    "        \n",
    "    file_root = sat + '_ubuff' + str(buffer_u) + '_rbuff' + str(buffer_r)\n",
    "    \n",
    "    # Check if should extract data\n",
    "    if (file_root not in files_already_extracted) | REEXTRACT_IF_FILE_EXISTS:\n",
    "        print(file_root)\n",
    "        \n",
    "        tasks_i = extract_satellite_by_year(survey_df, buffer_u, buffer_r, sat)\n",
    "        tasks_all.append(tasks_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Failed Tasks\n",
    "for task_list in tasks_all:\n",
    "    for task_i in task_list:\n",
    "        \n",
    "        task_i_status = task_i[0].status()\n",
    "        if task_i_status['state'] == 'FAILED':\n",
    "            print(task_i[0].status())\n",
    "            print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tasks not started\n",
    "for task_list in tasks_all:\n",
    "    for task_i in task_list:\n",
    "        \n",
    "        task_i_status = task_i[0].status()\n",
    "        if task_i_status['state'] == 'READY':\n",
    "            print(task_i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Task Q3GPOSLY7F2CSOBYLV23UJE7 EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2006_0 (UNSUBMITTED)>\n",
      "<Task ZQGKBM4YPHEPR4HIAEA26AQ5 EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2008_0 (UNSUBMITTED)>\n",
      "<Task QIPESQNGOYZRU654F2WZEO7P EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2009_0 (UNSUBMITTED)>\n",
      "<Task DAQV7L5APPT2HHVYQM5OMOPO EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2010_0 (UNSUBMITTED)>\n",
      "<Task 3NNCZ2BLATWQJBHSMUCTCEGC EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2011_1 (UNSUBMITTED)>\n",
      "<Task HSZ7UYKQNDVCUMHND4MI7AZH EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2012_1 (UNSUBMITTED)>\n",
      "<Task 4H4HBLSQ5DX4ATR7UQWM6OS4 EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2013_2 (UNSUBMITTED)>\n",
      "<Task 6OG7BY6EVI7MSXL7IFWOEGAK EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2014_2 (UNSUBMITTED)>\n",
      "<Task APIEAG4VH434UAZIGXL6TKSK EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2015_3 (UNSUBMITTED)>\n",
      "<Task DLQNC2LVVFSBXLXSUYFPJ5UU EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2016_10 (UNSUBMITTED)>\n",
      "<Task SZZJSGDEMIF7M2N7UMUAFCCW EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2017_10 (UNSUBMITTED)>\n",
      "<Task NL3Q3IUPD4D4ZRQRMWY5Q2IZ EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2018_11 (UNSUBMITTED)>\n",
      "<Task TR2IM7WBMOT7W5GT2ZT2MRQG EXPORT_FEATURES: elevation_ubuff5000_rbuff5000_2019_12 (UNSUBMITTED)>\n"
     ]
    }
   ],
   "source": [
    "## Tasks completed\n",
    "for task_list in tasks_all:\n",
    "    for task_i in task_list:\n",
    "        \n",
    "        task_i_status = task_i[0].status()\n",
    "        if task_i_status['state'] == 'COMPLETED':\n",
    "            print(task_i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n",
      "READY\n"
     ]
    }
   ],
   "source": [
    "## See all tasks\n",
    "for task_list in tasks_all:\n",
    "    for task_i in task_list:\n",
    "        \n",
    "        print(task_i[0].status()['state'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for task_list in tasks_all:\n",
    "        for task_i in task_list:\n",
    "\n",
    "            task_i[0].cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
