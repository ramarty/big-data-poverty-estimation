"sick from covid vaccine",                    "covid vaccine sick",
"does the covid vaccine make you sick",       "do you get sick after covid vaccine",
"covid vaccine second dose",                  "covid vaccine second dose sick",
"covid vaccine approved",                     "is covid vaccine approved",
"vaccine approval",                           "vaccine reaction",
"vaccine conspiracy",                         "vaccine allergy",
"vaccine toxins",
"vaccine mercury",                            "vaccine aluminum",
"vaccine dna", "vaccine",
"needle phobia",
"fear of needles",
"trypanophobia",
"covid vaccine reaction",
"covid vaccine near me",
"covid vaccine allergy") %>%
tolower()
KEYWORDS_TIMESERIES_ALL <- c(KEYWORDS_CONTAIN_USE, KEYWORDS_SYMTPOMS, VACCINE_KEYWORDS) %>% unique()
KEYWORDS_TIMESERIES_ALL_lw <- tolower(KEYWORDS_TIMESERIES_ALL)
KEYWORDS_SYMTPOMS_lw <- tolower(KEYWORDS_SYMTPOMS)
# Run Fresh --------------------------------------------------------------------
# Whether to run code "fresh"; that is, where all figures and processed/final
# data are deleted, except data downloaded from Google trends.
if(DELETE_OUTPUT){
TO_DELETE <- list.files(paper_figures, full.names = T)
for(file_i in TO_DELETE) file.remove(file_i)
TO_DELETE <- list.files(paper_tables, full.names = T)
for(file_i in TO_DELETE) file.remove(file_i)
}
if(DELETE_FINALDATA){
FINALDATA_FILES <- list.files(dropbox_file_path,
pattern = "*.Rds",
full.names = T,
recursive = T) %>%
str_subset("FinalData")
# Don't delete data downloaded from Google
DONT_DELETE <- file.path(prim_lang_dir, "FinalData", "gtrends_data") %>%
list.files(pattern = "*.Rds",
full.names = T)
TO_DELETE <- FINALDATA_FILES[!(FINALDATA_FILES %in% DONT_DELETE)]
for(file_i in TO_DELETE) file.remove(file_i)
}
# Scripts ----------------------------------------------------------------------
# List all variables in workspace until now. Before running some code below,
# delete everything except this and clear the memory to avoid a memory error.
ORIGINAL_VARIABLES <- NA
ORIGINAL_VARIABLES <- ls()
if(RUN_CODE){
# Process ancillary data -----------------------------------------------------
process_anc_dir <- file.path(datawork_dir, "01_process_ancillary_data")
## COVID Case Data
source(file.path(process_anc_dir, "01_covid_cases", "01_clean_data.R"))
## Oxford Policy Data
source(file.path(process_anc_dir, "01_oxford_policy", "01_download_data.R"))
source(file.path(process_anc_dir, "01_oxford_policy", "02_create_first_date_lockdown_data.R"))
source(file.path(process_anc_dir, "01_oxford_policy", "02_create_first_date_vaccine_avail.R"))
source(file.path(process_anc_dir, "01_oxford_policy", "02_ox_policy_national_clean.R"))
source(file.path(process_anc_dir, "01_oxford_policy", "02_vaccine_clean.R"))
## WDI
source(file.path(process_anc_dir, "01_wdi", "download_wdi_data.R"))
## Country Language Data
source(file.path(process_anc_dir, "02_countries_language_data", "01_clean_country_language_data.R"))
# Translate Google Search Terms ----------------------------------------------
if(TRANSLATE_GOOGLE_KEYWORDS){
source(file.path(datawork_dir, "02_translate_search_terms", "01_translate_search_terms.R"))
}
# Determine Most Common Language ---------------------------------------------
source(file.path(datawork_dir, "03_determine_most_common_language", "01_scrape_gtrends_mult_languages_per_country.R"))
source(file.path(datawork_dir, "03_determine_most_common_language", "02_append_data.R"))
# Scrape Google Trends Timeseries & US Data ----------------------------------
source(file.path(datawork_dir, "04_scrape_gtrends_data", "01_scrape_gtrends_global_timeseries.R"))
# Scrape Google Trends Data Across Terms in US -------------------------------
scrape_gtrends_us <- file.path(datawork_dir, "04_scrape_gtrends_us_data_across_terms")
source(file.path(scrape_gtrends_us, "01_scrape_clean.R"))
source(file.path(scrape_gtrends_us, "02a_scrape_clean_relative_ivermectin.R"))
source(file.path(scrape_gtrends_us, "02b_append_usa_refivermectin.R"))
# Clean Google Trends: Regional Data -----------------------------------------
source(file.path(datawork_dir, "05_clean_regions", "01_append_regional.R"))
source(file.path(datawork_dir, "05_clean_regions", "02_add_variables.R"))
# Clean Google Trends: Global Timeseries Data --------------------------------
# These scripts are memory intensive; before running each script, only keep
# needed variables and clear memory.
TO_DELETE <- ls()[!(ls() %in% ORIGINAL_VARIABLES)]
rm(TO_DELETE); gc(); gc()
source(file.path(datawork_dir, "05_clean_timeseries", "01_append_clean.R"))
TO_DELETE <- ls()[!(ls() %in% ORIGINAL_VARIABLES)]
rm(TO_DELETE); gc(); gc()
source(file.path(datawork_dir, "05_clean_timeseries", "02_merge_other_data.R"))
TO_DELETE <- ls()[!(ls() %in% ORIGINAL_VARIABLES)]
rm(TO_DELETE); gc(); gc()
source(file.path(datawork_dir, "05_clean_timeseries", "03_variable_construction.R"))
TO_DELETE <- ls()[!(ls() %in% ORIGINAL_VARIABLES)]
rm(TO_DELETE); gc(); gc()
source(file.path(datawork_dir, "05_clean_timeseries", "04_correlations.R"))
TO_DELETE <- ls()[!(ls() %in% ORIGINAL_VARIABLES)]
rm(TO_DELETE); gc(); gc()
# Analysis: Correlation with cases -------------------------------------------
# Boxplots showing distribution of correlations and best lags
# OUTPUTS:
# -- cor_lag_fig.png
# -- cor_corbest_fig.png
source(file.path(datawork_dir, "06_analysis", "main_correlations", "correlation_boxplots.R"))
# Figures showing trends in search interest & cases
# OUTPUTS:
# -- cases_vs_loss_of_smell_trends_topcountries.png
# -- cases_vs_loss_of_smell_trends_allcountries.png
source(file.path(datawork_dir, "06_analysis", "main_correlations", "cases_searchinterest_trends.R"))
# Map showing correlations across countries
# OUTPUT:
# -- cor_map.png
source(file.path(datawork_dir, "06_analysis", "main_correlations", "map.R"))
# Table of correlations and lags (for SI)
# OUTPUT:
# -- cor_lag_table.tex
source(file.path(datawork_dir, "06_analysis", "main_correlations", "correlation_table.R"))
# Regression explaining variation in correlation
# OUTPUT:
# -- lm_cor_loss_of_smell.tex
source(file.path(datawork_dir, "06_analysis", "main_correlations", "reg_explain_correlation.R"))
# Analysis: Impact of lockdowns ----------------------------------------------
# Lockdown analysis
# OUTPUT:
# -- did_pooled.png
source(file.path(datawork_dir, "06_analysis", "main_lockdowns", "01_lockdown_did_pooled.R"))
source(file.path(datawork_dir, "06_analysis", "main_lockdowns", "02_lockdown_did_pooled.R"))
source(file.path(datawork_dir, "06_analysis", "main_lockdowns", "03_lockdown_did_pooled.R"))
# Analysis: Vaccines ---------------------------------------------------------
# Global analysis of vaccines
# OUTPUT:
# -- vax_cor.png
source(file.path(datawork_dir, "06_analysis", "main_vaccines", "vaccine_global.R"))
# USA analysis of vaccines
# OUTPUT:
# -- vaccine_panels_[BEGIN_DATE]_[END_DATE].png
source(file.path(datawork_dir, "06_analysis", "main_vaccines", "vaccine_usa.R"))
}
#### Export: info on last code run
if(EXPORT_TXT_REPORT_CODE_DURATION){
END_TIME <- Sys.time()
sink(file.path(dropbox_file_path, "last_code_run_time.txt"))
cat("Details from latest time script was run \n")
cat("\n")
cat("START TIME: ", as.character(START_TIME), "\n", sep = "")
cat("END TIME: ", as.character(END_TIME), "\n", sep = "")
cat("DURATION: ",
difftime(END_TIME, START_TIME, units = "mins") %>%
as.numeric() %>%
round(2),
" minutes \n", sep = "")
cat("\n")
cat("PARAMETERS\n")
cat("RUN_CODE: ", RUN_CODE, "\n", sep = "")
cat("TRANSLATE_GOOGLE_KEYWORDS: ", TRANSLATE_GOOGLE_KEYWORDS, "\n", sep = "")
cat("DELETE_OUTPUT: ", DELETE_OUTPUT, "\n", sep = "")
cat("DELETE_FINALDATA: ", DELETE_FINALDATA, "\n", sep = "")
sink()
}
out <- gtrends("fever",
geo = "US",
time = "2021-04-06 2021-12-31",
onlyInterest = T,
low_search_volume=T)
out$interest_over_time %>% tail()
out <- gtrends("fever",
geo = "US",
time = "2021-04-05s 2021-12-31",
onlyInterest = T,
low_search_volume=T)
out$interest_over_time %>% tail()
out <- gtrends("fever",
geo = "US",
time = "2021-04-05 2021-12-31",
onlyInterest = T,
low_search_volume=T)
out$interest_over_time %>% tail()
out <- gtrends("fever",
geo = "US",
time = "2021-04-06 2021-12-31",
onlyInterest = T,
low_search_volume=T)
out$interest_over_time %>% tail()
## Timeframe to scrape
# -- [timeseries]_[begin date]_[end date] to scrape time series data for all
#   countries
# -- [timeseries_regions]_[begin date]_[end date] to only scrape data for the US
#    for select keywords and to capture data at the State level; does not
#    capture the time series
GTRENDS_TO_SCRAPE <- c("timeseries_2018-09-01_2019-05-28",
"timeseries_2019-01-01_2019-09-27",
"timeseries_2019-07-01_2020-03-26",
"timeseries_2020-01-01_2020-09-26",
"timeseries_2020-07-05_2021-03-31",
"timeseries_2021-01-04_2021-09-30",
"timeseries_2021-04-06_2021-12-31")
GTRENDS_TO_SCRAPE <- GTRENDS_TO_SCRAPE[7]
GTRENDS_TO_SCRAPE
# Poverty Estimation from Satellite Imagery in Pakistan
SURVEY_NAME <- "DHS"
# Root Directories -------------------------------------------------------------
#### Root Paths
# * Dropbox [dropbox_dir]: Where most data files are saved.
# * Google Drive [gdrive_dir]: We use Google Colab for processing ML models; here,
#   files for colab are saved in Google Drive
# * Secure [secure_dir]: Needed for PII data (World Bank OneDrive). Only needed
#   for OPM data
# * Github [github_dir]: Path to github repo
# * API Keys [api_key_dir]: Path where API keys are stored (api_keys.csv")
if(Sys.info()[["user"]] == "robmarty"){
dropbox_dir <- "~/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites"
gdrive_dir <- "~/Google Drive/World Bank/IEs/Pakistan Poverty Estimation"
secure_dir <- "~/Documents/World Bank/Pakistan Poverty from Sky"
github_dir <- "~/Documents/Github/Pakistan-Poverty-from-Sky"
overleaf_global_dir <- "~/Dropbox/Apps/Overleaf/Poverty Estimation - Global Paper"
overleaf_pak_dir <- "~/Dropbox/Apps/Overleaf/Poverty Estimation - Pakistan Paper"
}
if(Sys.info()[["user"]] == "WB521633"){
dropbox_dir <- "C:/Users/wb521633/Dropbox/World Bank/IEs/Pakistan Poverty Estimation from Satellites"
#gdrive_dir
secure_dir <- "C:/Users/wb521633/OneDrive - WBG/Pakistan Poverty from Sky - Survey Data"
github_dir <- "C:/Users/wb521633/Documents/GitHub/Pakistan-Poverty-from-Sky"
}
# Paths from Root --------------------------------------------------------------
#### Dropbox Paths
data_dir         <- file.path(dropbox_dir, "Data")
opm_dir          <- file.path(data_dir, "OPM")
osm_dir          <- file.path(data_dir, "OSM")
dhs_dir          <- file.path(data_dir, "DHS")
gadm_dir         <- file.path(data_dir, "GADM")
fb_marketing_dir <- file.path(data_dir, "Facebook Marketing")
fb_rwi_dir       <- file.path(data_dir, "Facebook Relative Wealth Index")
globcover_dir    <- file.path(data_dir, "Globcover")
worldclim_dir    <- file.path(data_dir, "WorldClim")
cntry_dtls_dir   <- file.path(data_dir, "Country Details")
#### Overleaf Paths
tables_global_dir  <- file.path(overleaf_global_dir, "tables")
figures_global_dir <- file.path(overleaf_global_dir, "figures")
tables_pak_dir  <- file.path(overleaf_pak_dir, "tables")
figures_pak_dir <- file.path(overleaf_pak_dir, "figures")
#### API Key Paths (For Facebook)
api_key_dir <- file.path(dropbox_dir, "API Keys")
#### Google Drive Paths
gdrive_cnn_file_path <- file.path(gdrive_dir, "Data", "CNN")
# Create Directory Structure for Survey Data -----------------------------------
survey_name_i <- "DHS"
for(survey_name_i in c("DHS", "OPM", "OPM_GPSDISP_DHS")){
### DROPBOX
file.path(data_dir, survey_name_i) %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData") %>% dir.create()
# FinalData/
file.path(data_dir, survey_name_i, "FinalData", "Individual Datasets") %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData", "Merged Datasets") %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData", "pov_estimation_results") %>% dir.create()
# FinalData/Individual Datasets
file.path(data_dir, survey_name_i, "FinalData", "Individual Datasets", "fb_mau_individual_datasets") %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData", "Individual Datasets", "globcover") %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData", "Individual Datasets", "osm") %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData", "Individual Datasets", "satellite_data_from_gee") %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData", "Individual Datasets", "worldclim") %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData", "Individual Datasets", "cnn_features") %>% dir.create()
# FinalData/Individual Datasets/osm
file.path(data_dir, survey_name_i, "FinalData", "Individual Datasets", "osm", "poi") %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData", "Individual Datasets", "osm", "roads_density") %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData", "Individual Datasets", "osm", "roads_distance") %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData", "pov_estimation_results", "predictions") %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData", "pov_estimation_results", "feature_importance") %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData", "pov_estimation_results", "accuracy") %>% dir.create()
file.path(data_dir, survey_name_i, "FinalData", "pov_estimation_results", "grid_search") %>% dir.create()
### GOOGLE DRIVE
file.path(gdrive_dir, "Data", survey_name_i) %>% dir.create()
file.path(gdrive_dir, "Data", survey_name_i, "FinalData") %>% dir.create()
# FinalData/
file.path(gdrive_dir, "Data", survey_name_i, "FinalData", "cnn_features") %>% dir.create()
file.path(gdrive_dir, "Data", survey_name_i, "FinalData", "cnn_models") %>% dir.create()
file.path(gdrive_dir, "Data", survey_name_i, "FinalData", "cnn_predictions") %>% dir.create()
file.path(gdrive_dir, "Data", survey_name_i, "FinalData", "Individual Datasets") %>% dir.create()
# FinalData/Individual Datasets/
file.path(gdrive_dir, "Data", survey_name_i, "FinalData", "Individual Datasets",
paste0("satellite_data_from_gee_", tolower(survey_name_i))) %>% dir.create()
file.path(gdrive_dir, "Data", survey_name_i, "FinalData", "Individual Datasets", "cnn_l8") %>% dir.create()
file.path(gdrive_dir, "Data", survey_name_i, "FinalData", "Individual Datasets", "cnn_l8", "tfrecords") %>% dir.create()
}
# Parameters -------------------------------------------------------------------
PAK_UTM_PROJ <- "+init=epsg:24313"
PK_UTM_PROJ <- PAK_UTM_PROJ
# Packages ---------------------------------------------------------------------
library(rgdal)
library(viridis)
library(readstata13)
library(dplyr)
library(data.table)
library(raster)
library(leaflet)
library(rgdal)
library(dplyr)
library(raster)
library(stringdist)
library(tmaptools)
library(stringr)
library(doBy)
library(rgeos)
library(haven)
library(alluvial)
library(ggmap)
library(velox)
library(sf)
library(sp)
library(raster)
library(rgeos)
library(tidyverse)
library(caret)
library(mltest)
library(RANN)
library(tidyverse)
library(lubridate)
library(jsonlite)
library(httr)
library(curl)
library(ggpmisc)
library(haven)
library(sjmisc)
library(dbscan)
library(ggplot2)
library(spatialEco)
library(geosphere)
library(radiant.data)
library(readxl)
library(osmar)
library(tidyverse)
library(lubridate)
library(jsonlite)
library(httr)
library(curl)
library(haven)
library(httr)
library(mclust)
library(missMDA)
library(DescTools)
library(FactoMineR)
library(countrycode)
library(furrr)
library(progressr)
library(ggmap)
library(ggridges)
library(ggpubr)
library(xgboost)
library(WDI)
library(ggtext)
library(gghalves)
library(ggthemes)
library(rnaturalearth)
library(ggrepel)
source(file.path(github_dir, "Functions", "functions.R"))
source("https://raw.githubusercontent.com/ramarty/fast-functions/master/R/functions_in_chunks.R")
source("https://raw.githubusercontent.com/ramarty/rSocialWatcher/main/R/main.R")
# Run Scripts ------------------------------------------------------------------
#### PARAMETERS
# Whether to re-run code that creates the dataset with Facebook parameters. This
# requires and API key, and may not work if Facebook updated the version of the
# API. For example, may get can error that says need to update the API to the
# latest version. If this happens, need to go to the Facebook developer
# platform and changed the API to the latest version.
RERUN_FB_CREATE_PARAM_DATASET <- F
# Whether to re-process the preparation of OSM files. This includes:
# (1) Converting from shp to Rds;
# (2) Splitting India by ADM2 region
# (3) Splitting OSM files into unique countries.
# These processes take a long time.
PREP_OSM_FILES <- F
# When extracting ancillary data to survey locations, whether to skip files that
# are already extracted. For example, if globcover data already extracted for
# Kenya, skip extracting for Kenya; only implement for countries+datasets where
# need to extract data.
REPLACE_IF_EXTRACTED <- F
#### FILE PATHS FOR CODE
datawork_dir <- file.path(github_dir, "DataWork")
anc_dir              <- file.path(datawork_dir, "02_get_process_ancillary_data")
anc_globcover_dir    <- file.path(anc_dir, "Globcover")
anc_worldclim_dir    <- file.path(anc_dir, "WorldClim")
anc_fb_marketing_dir <- file.path(anc_dir, "Facebook Marketing")
anc_fb_rwi_dir       <- file.path(anc_dir, "Facebook Relative Wealth Index")
anc_osm_dir          <- file.path(anc_dir, "OSM")
anc_satellite_dir    <- file.path(anc_dir, "Satellite Data")
anc_cnn_features_dir <- file.path(anc_dir, "CNN Features Predict NTL")
#### RUN CODE
if(F){
# 0. Download GADM -----------------------------------------------------------
# We use GADM across multiple files, so we download.
# -- 01_download_gadm.R: downloads GADM up to ADM2.
# -- 02_clean_adm2.R: cleans the data. Some countries don't have ADM2; in these
#    cases, to standardize the data, we assign the ADM2 name to just be the
#    ADM1 name.
source(file.path(datawork_dir, "00_download_gadm", "01_download_gadm.R"))
source(file.path(datawork_dir, "00_download_gadm", "02_clean_adm2.R"))
# 1. Clean DHS Data ----------------------------------------------------------
# Cleans DHS data. Appends DHS data across countries, prepares poverty
# variables (eg, asset indices) and aggregates to cluster level.
# -- 01_clean_dhs.R: Appends data across countries
# -- 02_clean_dhs_varconstruction.R: Consructs variables and aggregates to
#    cluster level.
source(file.path(datawork_dir, "01_clean_dhs", "01_clean_dhs.R"))
source(file.path(datawork_dir, "01_clean_dhs", "02_clean_dhs_varconstruction.R"))
# 1. Clean OPM Data ----------------------------------------------------------
# Cleans OPM data. Preps variables, cleans coordinates and aggregates to PSU.
source(file.path(datawork_dir, "01_clean_opm", "01_clean_opm.R"))
# 2. Get/Process Ancillary Data ----------------------------------------------
# Extract variables to survey locations (eg, satellite data, facebook data, etc)
# ** 2.1 Globcover -----------------------------------------------------------
# Extract globcover data; proportion of area near survey classified according
# to each class in globcover.
# -- 01_extract_globcover.R: Extracts data; saves data for each country
# -- 02_append.R Appends data for each country.
source(file.path(anc_globcover_dir, "01_extract_globcover.R"))
source(file.path(anc_globcover_dir, "02_append.R"))
# ** 2.1 WorldClim -----------------------------------------------------------
# Extract data from WorldClim (eg, temperature and precipitation)
# -- 01_extract_worldclim.R: Extracts data; saves data for each country
# -- 02_append.R Appends data for each country.
source(file.path(anc_worldclim_dir, "01_extract_worldclim.R"))
source(file.path(anc_worldclim_dir, "02_append.R"))
# ** 2.1 OSM -----------------------------------------------------------------
# Extracts data from OSM. Uses the points of interest (POI) and roads dataset
# to detemine (1) number of POI near survey locations, (2) distance to nearest
# POI (by type), (3) road density by road type and (4) distance to nearest
# road (by type)
# -- 01_osm_to_rds.R: OSM originally downloaded as shapefiles. These take
#    forever to load. Here, we load and save as .Rds files, which are faster
#    to load.
# -- 02_split_india_by_gadm.R: India has very large OSM files, which makes
#    processing difficult. Here, we break up files so that there's a file for
#    each ADM2 within India. When masking by ADM2, we buffer the ADM2 first.
#    When processing the data later, we first check which ADM2 the survey is
#    in the grab the relevant ADM2 OSM file for India.
# -- 02_split_to_unique_countries.R: Files are downloaded from http://www.geofabrik.de/,
#    which typically has data prepped for each country. Occasinally, they
#    combine countries into one dataset (eg, Haiti and Dominican Rep. in one
#    datast as opposed to two). Here, we split datasets so that there's one
#    dataset per country.
# -- 03_extract_poi.R: Extracts density and distance from POI dataset.
# -- 03_extract_road_density.R: Extracts road density near each survey location.
# -- 03_extract_road_distance.R: Extracts road distance near each survey location.
# -- 04_merge_data.R: Merges all OSM data together. Above, data is saved for
#    each country. This script appends those files and merges the POI and road
#    files together.
if(PREP_OSM_FILES){
source(file.path(anc_osm_dir, "01_osm_to_rds.R"))
source(file.path(anc_osm_dir, "02_split_india_by_gadm.R"))
source(file.path(anc_osm_dir, "02_split_to_unique_countries.R"))
}
source(file.path(anc_osm_dir, "03_extract_poi.R"))
source(file.path(anc_osm_dir, "03_extract_road_density.R"))
source(file.path(anc_osm_dir, "03_extract_road_distance.R"))
source(file.path(anc_osm_dir, "04_merge_data.R"))
# ** 2.1 Facebook Marketing Data ---------------------------------------------
# Extracts Facebook Marketing data around each survey location (monthly and
# daily active users across a number of characteristics).
# -- 01_search_behavior_interests_IDs.R: Creates dataset of behaviors and
#    interests and associated IDs. This dataset helps construct the API
#    requests for scraping data.
# -- 02_scrape_facebook_data.R: Scrapes Facebook Data; creates a dataset
#    for each survey location.
# -- 03_append_data.R: Appends the above data together. Also saves a dataset
#    of the parameters used when scraping data
# -- 04_clean_param_df.R: Cleans parameter dataframe, one of the datasets
#    created above. Makes interpretable categories; for example, instead of
#    indicating "scraped education levels 1,2,3,4...", say "up to high school"
if(RERUN_FB_CREATE_PARAM_DATASET){
source(file.path(anc_fb_marketing_dir, "01_search_behavior_interests_IDs.R"))
}
source(file.path(anc_fb_marketing_dir, "02_scrape_facebook_data.R"))
source(file.path(anc_fb_marketing_dir, "03_append_data.R"))
source(file.path(anc_fb_marketing_dir, "04_clean_param_df.R"))
# ** 2.1 Satellite Data ------------------------------------------------------
# Extracts satellite data from Google Earth Engine. For example, extracts
# nighttime lights, landsat data, etc.
# RUN THIS USING PYTHON!
#source(file.path(anc_fb_marketing_dir, "01_extract_values.ipynb"))
source(file.path(anc_fb_marketing_dir, "02_append_data.R"))
# ** 2.2 CNN Features Predict NTL --------------------------------------------
# Extracts features from CNN model that uses daytime imagery to predict NTL
# DEPENDS ON: 02_get_process_ancillary_data/Satellite Data/ being run first
# -- 01_create_ntlgroup_tfrecord_name.R: Create dataset that randomly picks
#    survey locations for CNN (creates balanced dataset across NTL values)
#    and groups locations together for different TF records. Adds in nighttime
#    lights value used for CNN.
# -- 02_extract_data_gee_for_cnn.ipynb: Extracts data used for CNN; matrix
#    of daytime imagery and corresponding NTL value. Outputs them as tfrecords.
# TODO:
# -- 03_[CNN FILE]: This is run on Google Colab
# -- 04_cnn_extract_features.ipynb: Extracts features from CNN model at each
#   survey location
source(file.path(anc_cnn_features_dir, "01_create_ntlgroup_tfrecord_name.R"))
#source(file.path(anc_cnn_features_dir, "02_extract_data_gee_for_cnn.ipynb"))
#source(file.path(anc_cnn_features_dir, "04_cnn_extract_features.ipynb"))
# 3. Merge Ancillary data with Survey ----------------------------------------
# Merges all ancillary data with survey data
# -- 01_merge_data.R: Merges data together
# -- 02_clean_data.R: Cleans data (variable construction, etc.) This script
#    creates the final data final that is used for poverty estimation.
source(file.path(datawork_dir, "03_merge_ancillary_data_with_survey", "01_merge_data.R"))
source(file.path(datawork_dir, "03_merge_ancillary_data_with_survey", "02_clean_data.R"))
# 4. Poverty Estimation ------------------------------------------------------
# Machine learning models for estimating poverty and creating datasets with
# results.
# --
# 5. Tables/Figures: Global analysis -----------------------------------------
# Makes tables and figures for paper
# --
figures_tables_global_dir <- file.path(datawork_dir, "05_figures_tables_global")
source(file.path(figures_tables_global_dir, "main", "00_correlations.R"))
source(file.path(figures_tables_global_dir, "main", "00_global_scatterplot_map.R"))
source(file.path(figures_tables_global_dir, "main", "01_avg_performance_by_type.R"))
}
setwd("~/Documents/Github/Pakistan-Poverty-from-Sky/DataWork/05_figures_tables_global/main")
