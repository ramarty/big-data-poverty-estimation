{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "import logging, os \n",
    "logging.disable(logging.WARNING) \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import config as cf\n",
    "import feature_extraction as fe\n",
    "\n",
    "### FOR REPRODUCIBILITY ###\n",
    "seed_value = 42\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "#tf.random.set_seed(seed_value)\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "#session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "#sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "#tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "CNN_FILENAME = os.path.join(cf.DROPBOX_DIRECTORY, 'Models', 'CNN', 'script_CNN.h5')\n",
    "#CNN_FILENAME = 'script_CNN.h5'\n",
    "FINAL_TARGET_NAME = 'ntl_bins'\n",
    "#CURRENT_DIRECTORY = cf.CURRENT_DIRECTORY\n",
    "VIIRS_GDF_FILEPATH = cf.VIIRS_GDF_FILEPATH\n",
    "DTL_DIRECTORY = cf.DTL_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_target(gdf, orig_target_name, n_bins):\n",
    "    '''\n",
    "    Creates log NTL variable and bins into 5 classes using k-means clutering.\n",
    "    '''\n",
    "    # Perform log(x+1) for defined domain\n",
    "    transformed_target_name = f'log_{orig_target_name}'\n",
    "    gdf[transformed_target_name] = np.log(gdf[orig_target_name] + 1)\n",
    "    # Bin target\n",
    "    target = gdf[transformed_target_name].to_numpy().reshape(-1,1)\n",
    "    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='kmeans')\n",
    "    gdf[FINAL_TARGET_NAME] = discretizer.fit_transform(target)\n",
    "\n",
    "\n",
    "def sample_by_target(input_gdf, target_col_name, n):\n",
    "    '''\n",
    "    Create a sample dataframe containing n observations from each target bin.\n",
    "    '''\n",
    "\n",
    "    gdf = gpd.GeoDataFrame()\n",
    "    for x in input_gdf[target_col_name].unique():\n",
    "        bin_gdf = input_gdf[input_gdf[target_col_name] == x]\n",
    "        sample_gdf = bin_gdf.sample(n=n, random_state=1)\n",
    "        gdf = gdf.append(sample_gdf)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def normalize(X):\n",
    "    '''\n",
    "    Normalizes features.\n",
    "    '''\n",
    "    return X.astype('float32') / 255.0\n",
    "\n",
    "\n",
    "def define_model(height, width, channels, num_classes):\n",
    "    '''\n",
    "    Defines and compiles CNN model.\n",
    "    \n",
    "    Inputs:\n",
    "        height, width, channels, num_classes (int)\n",
    "    Returns:\n",
    "        model (keras.Model object)\n",
    "    '''\n",
    "    # Define layers\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=64, \n",
    "                     kernel_size=(5, 5), \n",
    "                     activation='relu', \n",
    "                     input_shape=(height, width, channels),\n",
    "                     name='conv1'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='maxpool1'))\n",
    "    model.add(Flatten(name='flatten1'))\n",
    "    model.add(Dense(100, activation='relu', name='dense1'))\n",
    "    model.add(Dense(num_classes, activation='softmax', name='dense2'))\n",
    "    # Compile model\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def define_model_imagenet(height, width, channels, num_classes):\n",
    "    '''\n",
    "    Defines and compiles CNN model.\n",
    "    \n",
    "    Inputs:\n",
    "        height, width, channels, num_classes (int)\n",
    "    Returns:\n",
    "        model (keras.Model object)\n",
    "    '''\n",
    "\n",
    "    # https://medium.com/abraia/first-steps-with-transfer-learning-for-custom-image-classification-with-keras-b941601fcad5\n",
    "    # https://towardsdatascience.com/cnn-transfer-learning-fine-tuning-9f3e7c5806b2\n",
    "\n",
    "    #### Base model\n",
    "    input_shape = (height, width, channels)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #### Model Customization\n",
    "    # We take the last layer of our the model and add it to our classifier\n",
    "    last = base_model.layers[-1].output\n",
    "    x = Flatten()(last)\n",
    "    x = Dense(100, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    model = Model(base_model.input, x)\n",
    "    # We compile the model\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    #model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    #x = base_model.output\n",
    "    #x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    #x = Dropout(0.4)(x)\n",
    "    #x = Dense(100, activation='relu', name='dense1')(x)\n",
    "    #predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    #model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    #model.compile(optimizer='rmsprop',\n",
    "    #          loss='categorical_crossentropy',\n",
    "    #          metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, trainX, trainY, testX, testY):\n",
    "    '''\n",
    "    Fits model, evaluates model, saves best model over epochs and cross-validations.\n",
    "    \n",
    "    Inputs:\n",
    "        model (CNN model) keras.Model object\n",
    "        trainX, trainY (numpy.ndarray) 4D array of DTL features and 2D array of targets for training\n",
    "        testX, testY (numpy.ndarray) 4D array of DTL features and 2D array of targets for testing\n",
    "        current_kfold (int) iteration in kfold cross-val, default=None for no cross-val\n",
    "        display_metrics (bool) Default=False\n",
    "    Returns:\n",
    "        None\n",
    "    # https://towardsdatascience.com/step-by-step-guide-to-using-pretrained-models-in-keras-c9097b647b29\n",
    "    '''\n",
    "\n",
    "    # Use early stopping to help with overfitting\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=False)\n",
    "\n",
    "    # Save best model based on accuracy\n",
    "    mc = ModelCheckpoint(CNN_FILENAME, monitor='val_loss', mode='min', \n",
    "                         verbose=True, save_best_only=True)\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(trainX, trainY, \n",
    "            epochs=10, \n",
    "            batch_size=500, \n",
    "            validation_data=(testX, testY), \n",
    "            callbacks=[es, mc], \n",
    "            verbose=False)\n",
    "\n",
    "    # Show accuracy\n",
    "    loss, accuracy = model.evaluate(testX, testY, verbose=False)\n",
    "    print(f'                              Accuracy: {accuracy}')\n",
    "\n",
    "    #return model\n",
    "        \n",
    "\n",
    "def evaluate_with_crossval(model, dataX, dataY, k=2):\n",
    "    '''\n",
    "    Performs evaulation with K-fold cross validation.\n",
    "    \n",
    "    Inputs:\n",
    "        model (keras.Model object)\n",
    "        dataX, dataY (numpy.ndarray) 4D array of DTL features and 2D array of targets \n",
    "                                     for training\n",
    "        k (int)\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # Define k-fold cross-val\n",
    "    kfold = KFold(k, shuffle=True, random_state=1)\n",
    "    # Loop through folds\n",
    "    count = 1\n",
    "    for train_idx, test_idx in kfold.split(dataX):\n",
    "        print(f'{datetime.datetime.now()}    --- Current K-fold: {count} ---')\n",
    "        # Select subsets for training and testing\n",
    "        trainX, trainY, testX, testY = dataX[train_idx], dataY[train_idx], \\\n",
    "                                       dataX[test_idx], dataY[test_idx]\n",
    "        # Pass to evaluate_model function\n",
    "        evaluate_model(model, trainX, trainY, testX, testY)\n",
    "        count += 1\n",
    "\n",
    "\n",
    "def display_eval_metrics(model, testX, testY, n_ntl_bins):\n",
    "    '''\n",
    "    Displays evaluation metrics for a given trained model.\n",
    "    '''\n",
    "    # Get predictions\n",
    "    predY = model.predict(testX)\n",
    "    predY = np.argmax(predY, axis = 1)\n",
    "    testY_bins = np.argmax(testY, axis = 1)\n",
    "    # Generate classification report\n",
    "    classes = ['Radiance Level %01d' %i for i in range(1,n_ntl_bins+1)]\n",
    "    print(classification_report(testY_bins, predY, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS -------------------------------------------------------------\n",
    "\n",
    "# Process daytime and nighttime imagery for input into CNN? If False, uses \n",
    "# previously saved data\n",
    "reprocess_data = False\n",
    "\n",
    "# Daytime impage parameters\n",
    "image_height = 224 # VGG16 needs images to be rescale to 224x224\n",
    "image_width = 224\n",
    "bands = ['4', '3', '2'] # which bands to use? 4,3,2 are RGB\n",
    "\n",
    "N_bands = len(bands)\n",
    "\n",
    "# Number of bins for NTL\n",
    "n_ntl_bins = 3\n",
    "\n",
    "# Minimum observations to take from each NTL bin\n",
    "min_ntl_bin_count = 30\n",
    "\n",
    "#### Save parameters for later use\n",
    "cnn_param_dict = {'image_height': image_height, \n",
    "                'image_width': image_width,\n",
    "                'bands': bands,\n",
    "                'N_bands': N_bands,\n",
    "                'n_ntl_bins': n_ntl_bins,\n",
    "                'min_ntl_bin_count': min_ntl_bin_count}\n",
    "\n",
    "with open(cf.CNN_PARAMS_FILENAME, 'w') as fp:\n",
    "    json.dump(cnn_param_dict, fp)\n",
    "\n",
    "# Run --------------------------------------------------------------------\n",
    "\n",
    "if reprocess_data:\n",
    "\n",
    "    # LOAD DATA\n",
    "    print(f'{datetime.datetime.now()} 1. Load and Prep Data.')\n",
    "    viirs = pd.read_pickle(VIIRS_GDF_FILEPATH)\n",
    "    viirs_gdf = gpd.GeoDataFrame(viirs, geometry='geometry')\n",
    "    viirs_gdf = viirs_gdf[ ~ np.isnan(viirs_gdf['tile_id'])]\n",
    "\n",
    "    # PREP NTL\n",
    "    transform_target(viirs_gdf, 'median_rad_2014', n_ntl_bins)\n",
    "\n",
    "    # CREATE SAMPLE\n",
    "    min_bin_count = min(viirs_gdf[FINAL_TARGET_NAME].value_counts())\n",
    "    gdf = sample_by_target(viirs_gdf, FINAL_TARGET_NAME, min_ntl_bin_count)\n",
    "\n",
    "    # MATCH DTL TO NTL\n",
    "    print(f'{datetime.datetime.now()} 2. Matching DTL to NTL')\n",
    "\n",
    "    # input_gdf = gdf\n",
    "    # directory = DTL_DIRECTORY\n",
    "    # bands\n",
    "    # img_height = image_height\n",
    "    # img_width = image_width\n",
    "\n",
    "    DTL, processed_gdf = fe.map_DTL_NTL(gdf, DTL_DIRECTORY, bands, image_height, image_width)\n",
    "    NTL = processed_gdf[FINAL_TARGET_NAME].to_numpy()\n",
    "\n",
    "    np.save(os.path.join(cf.DROPBOX_DIRECTORY, 'Data', 'CNN - Processed Inputs', 'ntl.npy'), NTL)\n",
    "    np.save(os.path.join(cf.DROPBOX_DIRECTORY, 'Data', 'CNN - Processed Inputs' , 'dtl.npy'), DTL)\n",
    "\n",
    "else:\n",
    "\n",
    "    NTL = np.load(os.path.join(cf.DROPBOX_DIRECTORY, 'Data', 'CNN - Processed Inputs', 'ntl.npy'))\n",
    "    DTL = np.load(os.path.join(cf.DROPBOX_DIRECTORY, 'Data', 'CNN - Processed Inputs', 'dtl.npy'))\n",
    "\n",
    "# SPLIT DATA INTO TRAINING AND TESTING\n",
    "trainX, testX, raw_trainY, raw_testY = train_test_split(DTL, NTL, \n",
    "                                                        test_size=0.2)\n",
    "\n",
    "# DEFINE IMAGE CHARACTERISTICS\n",
    "#h, w, c, num_classes = 25, 25, 7, 5\n",
    "\n",
    "# PREP TRAINING AND TESTING DATA\n",
    "trainY = to_categorical(raw_trainY)\n",
    "testY = to_categorical(raw_testY)\n",
    "\n",
    "#print(f'{datetime.datetime.now()} 4. Prepping Training and Testing Sets.')\n",
    "#trainX, trainY = prep_dataset(raw_trainX, raw_trainY, image_height, image_width, N_bands)\n",
    "#testX, testY = prep_dataset(raw_testX, raw_testY, image_height, image_width, N_bands)\n",
    "\n",
    "print(np.unique(NTL, return_counts=True))\n",
    "\n",
    "print(np.unique(raw_trainY, return_counts=True))\n",
    "print(np.unique(raw_testY, return_counts=True))\n",
    "\n",
    "print(np.unique(trainY, return_counts=True))\n",
    "print(np.unique(testY, return_counts=True))\n",
    "\n",
    "# PREP PIXELS IN FEATURES\n",
    "trainX, testX = normalize(trainX), normalize(testX)\n",
    "\n",
    "# DEFINE AND EVALUTATE MODEL\n",
    "print(f'{datetime.datetime.now()} 5. Defining and Evaluating CNN.')\n",
    "\n",
    "#model = define_model(image_height, image_width, N_bands, n_ntl_bins)\n",
    "model = define_model_imagenet(image_height, image_width, N_bands, n_ntl_bins)\n",
    "\n",
    "#evaluate_with_crossval(model, trainX, trainY, k=5)\n",
    "evaluate_model(model, trainX, trainY, testX, testY)\n",
    "\n",
    "# DISPLAY IN-DEPTH EVALUTAION METRICS\n",
    "best_model = load_model(CNN_FILENAME)\n",
    "display_eval_metrics(model, testX, testY, n_ntl_bins)\n",
    "print(f'{datetime.datetime.now()} 6. END: CNN Saved.') \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
